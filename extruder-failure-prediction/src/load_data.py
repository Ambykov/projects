# load_data.py

import asyncio
import pandas as pd
from functools import reduce
from export_data_by_tag import export_data_by_tag
from clean_data import clean_dataframe
from calculate_features import calculate_features
from config import (
    TAGS,
    RESULT_DIR,
    TEST_LOAD_DATA,
    TEST_DATA_SEC,
    TEST_DATA_GROUP,
    TEST_DATA_DISCRET,
    TEST_DATA_CALC,
    SLEEP_INTERVAL,
    TIMEZONE_OFFSET,
    CHUNK_SECONDS,
    DATE_FORMAT,
    REQUIRED_FEATURES
)
import logging
import os
from datetime import datetime, timedelta, timezone
import numpy as np

logger = logging.getLogger(__name__)


async def fetch_tag_data(tag):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–≥–∞ –∏ –¥–æ–ø–∏—Å—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è"""
    try:
        df = export_data_by_tag(tag)  # ‚ùó –°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤
        if df is not None and not df.empty:
            logger.info(f"[INFO] üì• –ü–æ–ª—É—á–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ–≥–∞ {tag}: {df.shape[0]} —Å—Ç—Ä–æ–∫")

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º source_time –∫ datetime
            df['source_time'] = pd.to_datetime(df['source_time'])

            # –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            raw_file = os.path.join(RESULT_DIR, f"data_load_tag_{tag}.csv")

            # –ß–∏—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω—é—é –º–µ—Ç–∫—É –≤—Ä–µ–º–µ–Ω–∏ –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
            last_saved_time = None
            if os.path.exists(raw_file):
                # –ß—Ç–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤ –∏ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—Ç—Ä–æ–∫–∏
                try:
                    last_row = pd.read_csv(raw_file, parse_dates=['source_time'],
                                           usecols=['source_time'],
                                           tail=1)
                    if not last_row.empty:
                        last_saved_time = last_row.iloc[-1]['source_time']
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—Ç—Ä–æ–∫–∏: {e}")

            # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ: –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ –ø–æ–∑–∂–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–π
            if last_saved_time is not None:
                new_data = df[df['source_time'] > last_saved_time]
            else:
                new_data = df  # –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–æ–≤—ã–π –∏–ª–∏ –ø—É—Å—Ç–æ–π ‚Äî —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å—ë

            # –ï—Å–ª–∏ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç ‚Äî –≤—ã—Ö–æ–¥–∏–º
            if new_data.empty:
                logger.debug(f"[DEBUG] –ù–µ—Ç –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ–≥–∞ {tag}")
                return df[['source_time', f'value_{tag}']].copy()

            # –î–æ–ø–∏—Å—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            file_exists = os.path.exists(raw_file)
            new_data.to_csv(raw_file, index=False, mode='a', header=not file_exists)
            logger.debug(f"[DEBUG] ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π –¥–ª—è —Ç–µ–≥–∞ {tag}: {new_data.shape[0]}")

            return df[['source_time', f'value_{tag}']].copy()
        else:
            logger.warning(f"‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ–≥–∞ {tag}")
            return None
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ–≥–∞ {tag}: {e}")
        return None



def get_new_data_only(filepath, new_data):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Ç–µ —Å—Ç—Ä–æ–∫–∏ –∏–∑ new_data,
    —É –∫–æ—Ç–æ—Ä—ã—Ö source_time > –ø–æ—Å–ª–µ–¥–Ω–µ–π –º–µ—Ç–∫–∏ –≤ —Ñ–∞–π–ª–µ.
    """
    last_saved_time = None
    if os.path.exists(filepath):
        try:
            # –ß–∏—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ source_time –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—Ç—Ä–æ–∫–∏
            last_row = pd.read_csv(
                filepath,
                parse_dates=['source_time'],
                usecols=['source_time'],
                tail=1
            )
            if not last_row.empty:
                last_saved_time = pd.to_datetime(last_row.iloc[-1]['source_time'])
                logger.debug(f"[DEBUG] üïí –ü–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –≤ {filepath}: {last_saved_time}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—Ç—Ä–æ–∫–∏ –∏–∑ {filepath}: {e}")

    # –ü—Ä–∏–≤–æ–¥–∏–º source_time –≤ new_data –∫ datetime
    new_data['source_time'] = pd.to_datetime(new_data['source_time'])

    if last_saved_time is not None:
        filtered = new_data[new_data['source_time'] > last_saved_time]
        logger.debug(f"[DEBUG] ‚ú® –ù–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π –¥–ª—è {filepath}: {len(filtered)}")
        return filtered
    else:
        logger.debug(f"[DEBUG] üÜï –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π —Ñ–∞–π–ª: {filepath}")
        return new_data


async def process_tag_data(df, tag):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø–æ –æ–¥–Ω–æ–º—É —Ç–µ–≥—É:
    - –ß–∏—Å—Ç–∫–∞ –ø–æ —Å–µ–∫—É–Ω–¥–∞–º
    - –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞
    - –†–∞—Å—á—ë—Ç —Ñ–∏—á–µ–π
    """
    if df is None or df.empty:
        logger.warning(f"‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–≥–∞ {tag}")
        return None
    logger.info(f"‚öôÔ∏è –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–≥ {tag}")
    # –®–∞–≥ 1: —á–∏—Å—Ç–∫–∞ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ –ø–æ —Å–µ–∫—É–Ω–¥–∞–º
    cleaned_df = clean_dataframe(df, f'value_{tag}')
    logger.debug(f"[DEBUG] üìâ –ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–≥–∞ {tag}: {cleaned_df.head()}")
    logger.info(f"[INFO] üìè –†–∞–∑–º–µ—Ä cleaned_df –¥–ª—è —Ç–µ–≥–∞ {tag}: {cleaned_df.shape}")

    # –®–∞–≥ 2: –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ–ª–Ω—ã–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥ —Å —à–∞–≥–æ–º 1s
    full_range = pd.date_range(
        start=cleaned_df['source_time'].min(),
        end=cleaned_df['source_time'].max(),
        freq='s'
    )
    reindexed_df = cleaned_df.set_index('source_time').reindex(full_range).ffill().reset_index()
    reindexed_df.rename(columns={'index': 'source_time'}, inplace=True)
    logger.debug(f"[DEBUG] ‚è±Ô∏è –í—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –¥–ª—è —Ç–µ–≥–∞ {tag}: {reindexed_df.head()}")

    # –®–∞–≥ 3: –∑–∞–º–µ–Ω–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ "0"
    reindexed_df[f'value_{tag}'] = reindexed_df[f'value_{tag}'].clip(lower=0)
    logger.debug(f"[DEBUG] üìâ –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —Ç–µ–≥–∞ {tag} –∑–∞–º–µ–Ω–µ–Ω—ã –Ω–∞ 0: {reindexed_df[[f'value_{tag}']].head()}")


    # –®–∞–≥ 4: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ—Å–ª–µ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –ø–æ —Å–µ–∫—É–Ω–¥–∞–º (—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ)
    if TEST_DATA_SEC:
        sec_file = os.path.join(RESULT_DIR, f"data_sec_tag_{tag}.csv")
        debug_df = reindexed_df.copy()

        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        new_sec_data = get_new_data_only(sec_file, debug_df[['source_time', f'value_{tag}']].copy())

        if not new_sec_data.empty:
            file_exists = os.path.exists(sec_file)
            new_sec_data.to_csv(sec_file, index=False, mode='a', header=not file_exists)
            logger.debug(f"[DEBUG] ‚úÖ –ó–∞–ø–∏—Å–∞–Ω–æ {len(new_sec_data)} –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ –≤ {sec_file}")
        else:
            logger.debug(f"[DEBUG] ‚ö†Ô∏è –ù–µ—Ç –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–ø–∏—Å–∏ –≤ {sec_file}")


    # –®–∞–≥ 4: –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—è –ø–æ CHUNK_SECONDS
    reindexed_df['source_time'] = pd.to_datetime(reindexed_df['source_time'])
    discretized_df = reindexed_df.resample(f'{CHUNK_SECONDS}s', on='source_time').last().reset_index()

    numeric_cols = discretized_df.select_dtypes(include=np.number).columns
    discretized_df[numeric_cols] = discretized_df[numeric_cols].ffill().bfill()

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    if TEST_DATA_DISCRET:
        discret_file = os.path.join(RESULT_DIR, f"data_discrete_tag_{tag}.csv")
        debug_df = discretized_df[['source_time', f'value_{tag}']].copy()
        debug_df['source_time'] = debug_df['source_time'].dt.strftime(DATE_FORMAT)
        file_exists = os.path.exists(discret_file)
        debug_df.to_csv(discret_file, index=False, mode='a', header=not file_exists)
        logger.debug(f"[DEBUG] üìÅ –î–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏ –∑–∞–ø–∏—Å–∞–Ω—ã: {discret_file}")

    return discretized_df[['source_time', f'value_{tag}']]


async def load_all_tags_into_memory(stop_event):
    logger.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø–æ—Ç–æ–∫–æ–≤—É—é –∑–∞–≥—Ä—É–∑–∫—É –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏")
    while not stop_event.is_set():
        tasks = [fetch_tag_data(tag) for tag in TAGS]
        dfs = await asyncio.gather(*tasks)

        all_features_dfs = []

        for df, tag in zip(dfs, TAGS):
            if df is None or df.empty:
                logger.warning(f"‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ–≥–∞ {tag}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                continue

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ –æ–¥–Ω–æ–º—É —Ç–µ–≥—É
            discretized_df = await process_tag_data(df, tag)
            if discretized_df is None or discretized_df.empty:
                continue

            # –†–∞—Å—á—ë—Ç —Ñ–∏—á–µ–π
            value_col = f'value_{tag}'
            features_df = calculate_features(discretized_df, value_col)
            if features_df.empty:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å —Ñ–∏—á–∏ –¥–ª—è —Ç–µ–≥–∞ {tag}")
                continue

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏—á–µ–π (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            if TEST_DATA_CALC:
                calc_file = os.path.join(RESULT_DIR, f"data_calculated_tag_{tag}.csv")
                debug_df = features_df.copy()
                debug_df['source_time'] = pd.to_datetime(debug_df['source_time']).dt.strftime(DATE_FORMAT)
                file_exists = os.path.exists(calc_file)
                debug_df.to_csv(calc_file, index=False, mode='a', header=not file_exists)
                logger.debug(f"[DEBUG] üìÅ –§–∏—á–∏ –¥–ª—è —Ç–µ–≥–∞ {tag} –∑–∞–ø–∏—Å–∞–Ω—ã: {calc_file}")

            all_features_dfs.append(features_df)

        if not all_features_dfs:
            logger.warning("‚ö†Ô∏è –ù–µ—Ç —Ñ–∏—á–µ–π –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è")
            await asyncio.sleep(SLEEP_INTERVAL)
            continue

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ñ–∏—á–∏ –ø–æ source_time
        final_df = reduce(lambda left, right: pd.merge(left, right, on='source_time', how='outer'), all_features_dfs)
        final_df = final_df.loc[:, ~final_df.columns.duplicated()].copy()
        final_df.sort_values('source_time', inplace=True)
        final_df.reset_index(drop=True, inplace=True)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ñ–∏—á–µ–π
        missing_features = set(REQUIRED_FEATURES) - set(final_df.columns)
        if missing_features:
            logger.warning(f"[WARN] ‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ñ–∏—á–∏: {missing_features}. –ü—Ä–æ–≥–Ω–æ–∑ –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω.")
            logger.info("üîÑ –ñ–¥—ë–º –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
            await asyncio.sleep(SLEEP_INTERVAL)
            continue

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª calculated_features.csv
        output_path = os.path.join(RESULT_DIR, "calculated_features.csv")
        final_df_with_time = final_df[['source_time'] + REQUIRED_FEATURES].copy()
        final_df_with_time['source_time'] = pd.to_datetime(final_df_with_time['source_time']).dt.strftime(DATE_FORMAT)
        final_df_with_time['source_time'] = pd.to_datetime(final_df_with_time['source_time'])

        if os.path.exists(output_path):
            prev_df = pd.read_csv(output_path, parse_dates=['source_time'])
            combined_df = pd.concat([prev_df, final_df_with_time], ignore_index=True)
            combined_df = combined_df.drop_duplicates(subset=['source_time'], keep='last')
        else:
            combined_df = final_df_with_time.copy()

        combined_df.sort_values('source_time', inplace=True)
        combined_df.reset_index(drop=True, inplace=True)

        new_records_count = final_df_with_time.shape[0]
        total_records_count = combined_df.shape[0]

        try:
            combined_df.to_csv(output_path, index=False)
            logger.info(f"‚úÖ –ü—Ä–∏–∑–Ω–∞–∫–∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã: {output_path}")
            logger.info(f"üÜï –î–æ–±–∞–≤–ª–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {new_records_count} | üì¶ –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ —Ñ–∞–π–ª–µ: {total_records_count}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Å–∏ –≤ calculated_features.csv: {e}")

        await asyncio.sleep(SLEEP_INTERVAL)
    logger.info("üèÅ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")