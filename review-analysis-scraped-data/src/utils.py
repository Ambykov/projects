# src/utils.py

import logging
import pandas as pd
import numpy as np
import Levenshtein as lev
import re
from datetime import datetime

logger = logging.getLogger(__name__)

# --- –§—É–Ω–∫—Ü–∏—è –ø—Ä–æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∫–ª–∞—Å—Å–∞ ---
def set_class(df, idx, code):
    current_class = df.at[idx, '–ö–ª–∞—Å—Å']

    # –ï—Å–ª–∏ —Ç–µ–∫—É—â–∏–π –∫–ª–∞—Å—Å None –∏–ª–∏ –Ω–µ —Å–ø–∏—Å–æ–∫ ‚Üí —Å—Ç–∞–≤–∏–º [code]
    if not isinstance(current_class, list):
        df.at[idx, '–ö–ª–∞—Å—Å'] = [code]

    else:
        # –ï—Å–ª–∏ –µ—Å—Ç—å 0 ‚Äî –∑–∞–º–µ–Ω—è–µ–º –µ–≥–æ –Ω–∞ code
        if 0 in current_class:
            new_class = [code if c == 0 else c for c in current_class]
        # –ò–Ω–∞—á–µ –¥–æ–±–∞–≤–ª—è–µ–º, –µ—Å–ª–∏ –µ—â—ë –Ω–µ –±—ã–ª–æ
        elif code not in current_class:
            new_class = current_class + [code]
        else:
            new_class = current_class

        df.at[idx, '–ö–ª–∞—Å—Å'] = new_class

    return df


# --- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º ---
def class_statistics(df):
    normalized = df['–ö–ª–∞—Å—Å'].apply(tuple)
    stats = normalized.value_counts().reset_index()
    stats.columns = ['–ö–ª–∞—Å—Å', '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ']
    stats['–ö–ª–∞—Å—Å'] = stats['–ö–ª–∞—Å—Å'].apply(lambda x: list(x))
    total = len(df)
    stats['–ü—Ä–æ—Ü–µ–Ω—Ç'] = (stats['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ'] / total * 100).round(2)
    stats.loc[len(stats)] = ['–í—Å–µ–≥–æ', total, 100.00]
    return stats.reset_index(drop=True)


# --- –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ ---
def tokenize(texts, tokenizer, max_length):
    return tokenizer(
        texts,
        padding=True,
        truncation=True,
        max_length=max_length,
        return_tensors="pt"
    ).to('cpu')


# --- –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏ ---
def split_text_into_chunks(text, tokenizer, max_length, stride):
    tokens = tokenizer.encode(text, add_special_tokens=False)
    chunks = []
    start = 0
    while start < len(tokens):
        end = start + max_length
        chunk_tokens = tokens[start:end]
        chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True, clean_up_tokenization_spaces=True)
        chunks.append(chunk_text)
        start += stride
    return chunks


# --- –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ ---
def clean_text_columns(df):
    def clean(text):
        if not isinstance(text, str) or not text.strip():
            return ""
        text = text.strip()
        text = text.lower()
        # –ó–∞–º–µ–Ω–∞ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤ –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏
        text = re.sub(r'(?<=[–∞-—è–ê-–Ø])([^\w\s\/])(?=[–∞-—è–ê-–Ø])', ' ', text)
        # –£–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    df['–û—Ç–∑—ã–≤'] = df['–û—Ç–∑—ã–≤'].apply(clean)

    logger.info("‚úÖ –¢–µ–∫—Å—Ç–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –æ—á–∏—â–µ–Ω—ã")
    return df


# --- –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤ (–∫–ª–∞—Å—Å 101) ---
def is_short_symbol(text):
    if not isinstance(text, str) or not text.strip():
        return False

    text = text.strip()

    # –£–±–∏—Ä–∞–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –º–µ–∂–¥—É –±—É–∫–≤–∞–º–∏
    normalized = re.sub(r'(?<=[–∞-—è–ê-–Ø])([^\w\s\/])(?=[–∞-—è–ê-–Ø])', ' ', text)
    normalized = re.sub(r'\s+', ' ', normalized).strip()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É
    if not (1 <= len(normalized) <= 3):
        return False

    if normalized.isspace():
        return False

    # –ò—Å–∫–ª—é—á–∞–µ–º "–±—É", "–ë–£", "–±/—É" –∏ —Ç.–¥.
    cleaned_words = normalized.lower().replace(' ', '').replace('/', '')
    if cleaned_words == '–±—É':
        return False

    return True


def is_fuzzy_match_with_details(review_text, key_phrases):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –µ—Å—Ç—å –ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ —Ñ—Ä–∞–∑—ã —Å –æ–ø–µ—á–∞—Ç–∫–∞–º–∏ –∏–∑ key_phrases.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
        - —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫
        - —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∫–ª—é—á–æ–º 'phrases'

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        bool: –Ω–∞–π–¥–µ–Ω–æ –ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        list: —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (original, corrected, source)
    """
    matches = []

    review_words = review_text.split()

    for item in key_phrases:
        # –ï—Å–ª–∏ —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å —Å —Ñ—Ä–∞–∑–∞–º–∏
        if isinstance(item, dict):
            phrases = item.get("phrases", [])
        else:
            phrases = [item]

        for phrase in phrases:
            phrase_words = phrase.split()
            n = len(phrase_words)
            if n > len(review_words):
                continue

            # –ü–æ–∏—Å–∫ –≤ —Ç–µ–∫—Å—Ç–µ –≤—Å–µ—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –¥–ª–∏–Ω—ã n
            for start_pos in range(len(review_words) - n + 1):
                segment = review_words[start_pos : start_pos + n]

                match = True
                original_words = []
                corrected_words = []

                for i, target_word in enumerate(phrase_words):
                    word = segment[i]
                    original_words.append(word)

                    # –ü–æ–ª–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                    if word == target_word:
                        corrected_words.append(word)
                        continue

                    # –ó–∞–º–µ–Ω–∞ "—ë" ‚Üî "–µ"
                    if word.replace("—ë", "–µ") == target_word.replace("—ë", "–µ"):
                        corrected_words.append(word)
                        continue

                    # –°–ª–æ–≤–∞ –¥–ª–∏–Ω–æ–π 1‚Äì3 —Å–∏–º–≤–æ–ª–∞ ‚Üí —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∏–ª–∏ –∑–∞–º–µ–Ω–∞ "–Ω–µ" ‚Üî "–Ω–∏"
                    if len(target_word) <= 3:
                        if (target_word == "–Ω–µ" and word == "–Ω–∏") or (target_word == "–Ω–∏" and word == "–Ω–µ"):
                            corrected_words.append(word)
                        else:
                            match = False
                            break

                    # –°–ª–æ–≤–∞ 4‚Äì5 –±—É–∫–≤ ‚Üí –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω ‚â§1
                    elif 4 <= len(target_word) <= 5:
                        if lev.distance(word, target_word) <= 1:
                            corrected_words.append(word)
                        else:
                            match = False
                            break

                    # –°–ª–æ–≤–∞ >5 –±—É–∫–≤ ‚Üí –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω ‚â§2
                    else:
                        if lev.distance(word, target_word) <= 2:
                            corrected_words.append(word)
                        else:
                            match = False
                            break

                if match:
                    corrected_phrase = ' '.join(corrected_words)
                    phrase_joined = ' '.join(phrase_words)

                    # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–∏–ª–∏ —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º, –∏–ª–∏ —Ç–æ—á–Ω–æ–µ)
                    matches.append((
                        ' '.join(original_words),
                        corrected_phrase,
                        phrase_joined
                    ))
                    break  # –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —ç—Ç–æ–π —Ñ—Ä–∞–∑—ã –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–π

    return len(matches) > 0, matches




# --- –§—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ –±–ª–∏–∑–∫–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ñ—Ä–∞–∑—ã ---
def is_fuzzy_match_phrase_level(review_words, key_phrases):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –æ—Ç–∑—ã–≤ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ñ—Ä–∞–∑–µ, —Å –¥–æ–ø—É—Å—Ç–∏–º—ã–º–∏ –æ—à–∏–±–∫–∞–º–∏

    –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞:
        - 1‚Äì3 –±—É–∫–≤—ã ‚Üí —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∏–ª–∏ –∑–∞–º–µ–Ω–∞ "–µ"/"—ë", "–Ω–µ"/"–Ω–∏"
        - 4‚Äì5 –±—É–∫–≤ ‚Üí –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω ‚â§1
        - >5 –±—É–∫–≤ ‚Üí –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω ‚â§2

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        bool: –µ—Å—Ç—å –ª–∏ –ø–æ–¥—Ö–æ–¥—è—â–∞—è —Ñ—Ä–∞–∑–∞
        list: —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (original, corrected, source)
    """
    matches = []

    for item in key_phrases:
        # –ï—Å–ª–∏ —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å, –∏—â–µ–º –≤–æ —Ñ—Ä–∞–∑–∞—Ö
        if isinstance(item, dict):
            for phrase in item.get("phrases", []):
                found, submatches = _check_phrase(review_words, phrase)
                if found:
                    matches.extend(submatches)
        else:
            # –ï—Å–ª–∏ —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ —Å—Ç—Ä–æ–∫–∞
            found, submatches = _check_phrase(review_words, item)
            if found:
                matches.extend(submatches)

    return len(matches) > 0, matches


def _check_phrase(review_words, phrase):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–¥–Ω–æ–π —Ñ—Ä–∞–∑—ã"""
    phrase_words = phrase.split()
    if abs(len(review_words) - len(phrase_words)) > 1:
        return False, []

    original_words = []
    corrected_words = []
    match = True

    for i, target_word in enumerate(phrase_words):
        if i >= len(review_words):
            return False, []

        word = review_words[i]
        original_words.append(word)

        # –ü–æ–ª–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if word == target_word:
            corrected_words.append(word)
            continue

        # –ó–∞–º–µ–Ω–∞ "—ë" ‚Üî "–µ"
        if word.replace("—ë", "–µ") == target_word.replace("—ë", "–µ"):
            corrected_words.append(word)
            continue

        # –°–ª–æ–≤–∞ –¥–ª–∏–Ω–æ–π 1‚Äì3 —Å–∏–º–≤–æ–ª–∞ ‚Üí —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∏–ª–∏ "–Ω–µ" ‚Üî "–Ω–∏"
        if len(target_word) <= 3:
            if target_word == "–Ω–µ" and word == "–Ω–∏":
                corrected_words.append(word)
            elif target_word == "–Ω–∏" and word == "–Ω–µ":
                corrected_words.append(word)
            else:
                match = False
                break

        # –°–ª–æ–≤–∞ 4‚Äì5 –±—É–∫–≤ ‚Üí –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω ‚â§1
        elif 4 <= len(target_word) <= 5:
            if lev.distance(word, target_word) <= 1:
                corrected_words.append(word)
            else:
                match = False
                break

        # –°–ª–æ–≤–∞ >5 –±—É–∫–≤ ‚Üí –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω ‚â§2
        else:
            if lev.distance(word, target_word) <= 2:
                corrected_words.append(word)
            else:
                match = False
                break

    corrected_phrase = ' '.join(corrected_words)
    phrase_joined = ' '.join(phrase.split())

    if match and corrected_phrase != phrase_joined:
        return True, [(
            ' '.join(original_words),
            corrected_phrase,
            phrase_joined
        )]
    else:
        return False, []


##################################################################################
from decorators import pipeline_step
from config import NEGATIV_LEVEL, FILTERED_OUT_CLASS


@pipeline_step(step_number=2, step_name="–§–ò–õ–¨–¢–†–ê–¶–ò–Ø –ü–û NEGATIV_LEVEL")
def filter_by_negativ_level(df, stop_pipeline_flag=False, negativ_level=None):
    """
    –§–∏–ª—å—Ç—Ä—É–µ—Ç –¥–∞—Ç–∞—Ñ—Ä–µ–π–º: –æ—Ç–∑—ã–≤—ã —Å –æ—Ü–µ–Ω–∫–æ–π > negativ_level –Ω–µ —É–¥–∞–ª—è—é—Ç—Å—è,
    –∞ –ø–æ–º–µ—á–∞—é—Ç—Å—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º –∫–ª–∞—Å—Å–æ–º FILTERED_OUT_CLASS.

    :param df: –í—Ö–æ–¥–Ω–æ–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º —Å –∫–æ–ª–æ–Ω–∫–æ–π '–û—Ü–µ–Ω–∫–∞'
    :param stop_pipeline_flag: –§–ª–∞–≥ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è —à–∞–≥–∞
    :param negativ_level: –ü–æ—Ä–æ–≥ –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
    :return: –û–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º
    """
    if stop_pipeline_flag:
        logger.warning("üîö [!] –®–∞–≥ [2] –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        return df

    if df is None or df.empty:
        logger.warning("üü° [2] –í—Ö–æ–¥–Ω–æ–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –ø—É—Å—Ç–æ–π –∏–ª–∏ None")
        return df

    if negativ_level is None:
        negativ_level = NEGATIV_LEVEL

    logger.info(f"üîç [2] –ü—Ä–∏–º–µ–Ω—è—é —Ñ–∏–ª—å—Ç—Ä: –æ—Ü–µ–Ω–∫–∏ > {negativ_level} –±—É–¥—É—Ç –∏—Å–∫–ª—é—á–µ–Ω—ã –∏–∑ –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–ø–æ–º–µ—á–µ–Ω—ã –∫–ª–∞—Å—Å–æ–º {FILTERED_OUT_CLASS})")

    if '–û—Ü–µ–Ω–∫–∞' not in df.columns:
        logger.error("‚ùå [2] –ö–æ–ª–æ–Ω–∫–∞ '–û—Ü–µ–Ω–∫–∞' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ")
        raise ValueError("–ö–æ–ª–æ–Ω–∫–∞ '–û—Ü–µ–Ω–∫–∞' –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")

    # –ü—Ä–∏–≤–æ–¥–∏–º –æ—Ü–µ–Ω–∫—É –∫ —á–∏—Å–ª–æ–≤–æ–º—É —Ç–∏–ø—É
    df['–û—Ü–µ–Ω–∫–∞'] = pd.to_numeric(df['–û—Ü–µ–Ω–∫–∞'], errors='coerce')

    # –û—Ç–º–µ—Ç–∏–º –æ—Ç–∑—ã–≤—ã —Å –æ—Ü–µ–Ω–∫–æ–π > negativ_level —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º –∫–ª–∞—Å—Å–æ–º
    filtered_out_mask = df['–û—Ü–µ–Ω–∫–∞'] > negativ_level
    filtered_out_indices = df[filtered_out_mask].index

    logger.info(f"üü° [2] –û—Ç–º–µ—Ç–∫–∞ {len(filtered_out_indices)} –æ—Ç–∑—ã–≤–æ–≤ –∫–∞–∫ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–ª–æ–Ω–∫—É '–ö–ª–∞—Å—Å', –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
    if '–ö–ª–∞—Å—Å' not in df.columns:
        df['–ö–ª–∞—Å—Å'] = [[] for _ in range(len(df))]
    else:
        # –ü—Ä–∏–≤–æ–¥–∏–º –≤—Å–µ—Ö –∫ —Å–ø–∏—Å–∫—É –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–∏—è
        def ensure_list(x):
            if isinstance(x, list):
                return x
            elif pd.isna(x):
                return []
            else:
                return [x]
        df['–ö–ª–∞—Å—Å'] = df['–ö–ª–∞—Å—Å'].apply(ensure_list)

    # –ü—Ä–æ—Å—Ç–∞–≤–ª—è–µ–º FILTERED_OUT_CLASS –¥–ª—è –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
    for idx in filtered_out_indices:
        df = set_class(df, idx, FILTERED_OUT_CLASS)

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –±–µ–∑ —É–¥–∞–ª–µ–Ω–∏—è —Å—Ç—Ä–æ–∫ ‚Äî –±–ª–∞–≥–æ–¥–∞—Ä—è –∫–ª–∞—Å—Å–∞–º —Ñ–∏–ª—å—Ç—Ä –º–æ–∂–Ω–æ –æ–±—Ö–æ–¥–∏—Ç—å
    logger.info(f"‚úÖ [2] –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(filtered_out_indices)} –æ—Ç–∑—ã–≤–æ–≤ –ø–æ–º–µ—á–µ–Ω–æ –∫–ª–∞—Å—Å–æ–º {FILTERED_OUT_CLASS}")
    return df



from decorators import pipeline_step
from config import logger
import re

@pipeline_step(step_number=7, step_name="–û–ß–ò–°–¢–ö–ê –¢–ï–ö–°–¢–ê")
def step_clean_text(df, stop_pipeline_flag=False):
    import re
    logger.info("üßπ [7] –û–ß–ò–°–¢–ö–ê –¢–ï–ö–°–¢–ê: —Å—Ç–∞—Ä—Ç")

    if stop_pipeline_flag:
        logger.warning("üîö [!] –®–∞–≥ –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        return df

    if df is None or df.empty:
        logger.warning("üü° [7] –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ø—Ä–æ–ø—É—â–µ–Ω–∞ ‚Äî –≤—Ö–æ–¥–Ω–æ–π DataFrame –ø—É—Å—Ç")
        return df  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º df, –∞ –Ω–µ None

    if '–û—Ç–∑—ã–≤' not in df.columns:
        logger.error("‚ùå –ù–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ '–û—Ç–∑—ã–≤' –¥–ª—è –æ—á–∏—Å—Ç–∫–∏")
        return df  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º df, –∞ –Ω–µ None

    def clean(text):
        if not isinstance(text, str) or not text.strip():
            return ""

        text = text.strip().lower()

        # –ó–∞–º–µ–Ω—è–µ–º –≤—Å–µ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è, —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã –∏ —ç–º–æ–¥–∑–∏ –Ω–∞ –ø—Ä–æ–±–µ–ª—ã
        # –í–∫–ª—é—á–∞–µ–º –≤ –∫–ª–∞—Å—Å —Å–∏–º–≤–æ–ª–æ–≤:
        # - –≤—Å–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∑–Ω–∞–∫–∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
        # - –ª—é–±—ã–µ —Å–∏–º–≤–æ–ª—ã –∫—Ä–æ–º–µ –±—É–∫–≤, —Ü–∏—Ñ—Ä, –ø—Ä–æ–±–µ–ª–æ–≤ –∏ /
        # –£–¥–∞–ª—è–µ–º —Ç–∞–∫–∂–µ —ç–º–æ–¥–∑–∏ ‚Äî –∫–ª–∞—Å—Å —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ –Æ–Ω–∏–∫–æ–¥–∞
        # –î–ª—è —ç–º–æ–¥–∑–∏ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–∏–∞–ø–∞–∑–æ–Ω—ã Unicode, –∑–¥–µ—Å—å –æ–±—â–∏–π –ø—Ä–∏–º–µ—Ä

        # Unicode –¥–∏–∞–ø–∞–∑–æ–Ω—ã –¥–ª—è —ç–º–æ–¥–∑–∏, —Å–º–∞–π–ª–æ–≤ –∏ –¥—Ä.
        emoji_pattern = re.compile(
            "["
            "\U0001F600-\U0001F64F"  # —ç–º–æ—Ü–∏–∏ —Å–º–∞–π–ª—ã
            "\U0001F300-\U0001F5FF"  # —Å–∏–º–≤–æ–ª—ã –∏ –ø–∏–∫—Ç–æ–≥—Ä–∞–º–º—ã
            "\U0001F680-\U0001F6FF"  # —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç –∏ –∫–∞—Ä—Ç—ã
            "\U0001F700-\U0001F77F"  # –∞–ª—Ö–∏–º–∏—á–µ—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã
            "\U0001F780-\U0001F7FF"  # –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã
            "\U0001F800-\U0001F8FF"  # –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∏ —Å—Ç—Ä–µ–ª–∫–∏
            "\U0001F900-\U0001F9FF"  # –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –∫ —Å–º–∞–π–ª–∞–º
            "\U0001FA00-\U0001FA6F"  # –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –∫ —Å–∏–º–≤–æ–ª–∞–º
            "\U0001FA70-\U0001FAFF"
            "\U00002700-\U000027BF"  # —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
            "\U000024C2-\U0001F251"
            "]+", flags=re.UNICODE)
        text = emoji_pattern.sub(" ", text)

        # –ó–∞–º–µ–Ω—è–µ–º –≤—Å–µ —Å–∏–º–≤–æ–ª—ã –∫—Ä–æ–º–µ –±—É–∫–≤, —Ü–∏—Ñ—Ä, –ø—Ä–æ–±–µ–ª–æ–≤ –∏ / –Ω–∞ –ø—Ä–æ–±–µ–ª
        text = re.sub(r"[^a-z–∞-—è—ë0-9\s/]", " ", text, flags=re.IGNORECASE)

        # –ó–∞–º–µ–Ω—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–±–µ–ª–æ–≤ –Ω–∞ –æ–¥–∏–Ω –∏ —É–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞—á–∞–ª–µ/–∫–æ–Ω—Ü–µ
        text = re.sub(r"\s+", " ", text).strip()

        return text

    df['–û—Ç–∑—ã–≤'] = df['–û—Ç–∑—ã–≤'].apply(clean)

    logger.info("‚úÖ [7] –û–ß–ò–°–¢–ö–ê –¢–ï–ö–°–¢–ê: –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
    return df
