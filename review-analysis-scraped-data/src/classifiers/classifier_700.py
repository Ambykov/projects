# src/classifiers/classifier_700.py

import os
import pandas as pd
from tqdm import tqdm
import Levenshtein as lev


from config import logger, KEYWORDS_FILES, SAVE_STEP_10_RESULT, PROCESSED_DIR, TIMESTAMP
from utils import set_class
from utils import is_fuzzy_match_with_details as base_is_fuzzy_match_with_details
from decorators import pipeline_step



def has_ignored_class(classes):
    if isinstance(classes, list):
        return 999 in classes or 100 in classes
    return classes == 999 or classes == 100



# –°–ª–æ–≤–∞—Ä—å –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
ALLOWED_EXCEPTIONS = {
    '—Å–∞–Ω–∫—Ü–∏–∏':  ['—Å–µ–∫—Ü–∏–∏', '—Ñ—É–Ω–∫—Ü–∏–∏', '–∞–∫—Ü–∏–∏', '—Å—Ç–∞–Ω—Ü–∏–∏'],
    '–Ω–∞—Ç–æ–≤–µ—Ü':  ['–Ω–∞–∫–æ–Ω–µ—Ü'],
    '–Ω–∞—Ç–æ':     ['–Ω–∞–¥–æ', '–Ω–µ—Ç–æ', '–∑–∞—Ç–æ', '–Ω–∞—Ç–æ—á–µ–Ω'],
    '—Å—Ç–∞–ª–∏–Ω':   ['—Å—Ç–∞–ª–æ', '—Å—Ç–∞–ª–∞', '—Å—Ç–∞–ª', '—Å—Ç–∞–ª–∏', '—Å—Ç–∞–¥–∏–∏', '—Å–¥–∞–ª–∏', '—Å—Ç–∞–∫–∞–Ω', '—Å—Ç–∞–≤–∏—Ç', '—Å—Ç–∞–≤–∏–ª', '—Å–ø–∞–ª–∏',
                 '—Å—Ç–∞–ª—å', '—É—Å—Ç–∞–ª–∏', '—Ç–∞–ª–æ–Ω', '–≤—Å—Ç–∞–ª–∏', '—Å—Ç–∞–≤–∏–º', '—Å—Ç–∞—Ç–∏', '—Å—Ç–æ–ª–∏–∫'],
    '—Å–ª–µ–¥–∫–æ–º':  ['—Å–ª–∏—à–∫–æ–º', '—Å–ª–µ–¥–æ–º', '–ª–µ–≥–∫–æ–º', '—Ä–µ–¥–∫–æ–º', '—Å–ª–∞–¥–∫–æ–π'],
    '–º–æ–¥–∏':     ['–º–æ–∏', '–º–æ–ª–∏', '–ø–æ–¥–∏'],
    '–ø—É—Ç–∏–Ω':    ['–ø—É—Ç–∏'],
    '–ø—É—Ç–µ–Ω':    ['–ø—É—Ç–µ–º', '–ø—è—Ç–µ–Ω'],
    '–º–∏–Ω—Ñ–∏–Ω':   ['–º–∏–Ω–∏', '–º–∏–Ω–∏–Ω'],
    '—Å–µ–Ω–∞—Ç–æ—Ä':  ['—Å–µ–∫–∞—Ç–æ—Ä', '—Å–µ–∫–∞—Ç–æ—Ä—ã', '—Å–∏–∫–∞—Ç–æ—Ä', '—Å–µ–Ω—Å–æ—Ä', '—Å–µ–∫—Ç–æ—Ä'],
    '–º–∞–∫—Ä–æ–Ω':   ['–º–∞–∫–∞—Ä–æ–º', '–Ω–∞–∫–ª–æ–Ω', '–∑–∞–∫–æ–Ω', '–º–æ–∫—Ä–æ–µ', '–º–æ–∫—Ä–∞—è', '–º–∞–∫–∞—Ä–æ–Ω—ã', '–º–∏–∫—Ä–æ–Ω', '–º–∞—Ä–æ–∫', '–ø–∞—Ç—Ä–æ–Ω'],
    '—Ç–µ—Ä–∞–∫—Ç':   ['—Ç–µ—Ä—è–µ—Ç', '—Ç–µ—Ä–∫–∞', '—Ç–µ—Ä–∫–∏', '—Ç–µ—Ä–∫—É', '—Ç–µ—Ä–∫–µ'],
    '–ø—Ä–æ—Ç–µ—Å—Ç—ã': ['–ø—Ä–æ—Ç–µ—Ä—Ç–∞', '–ø–æ—Ç–µ—Ä—Ç—ã', '–ø—Ä–æ–≤–µ—Å—Ç–∏', '–ø—Ä–æ—Ü–µ–Ω—Ç—ã'],
    '–º–∞—Ñ–∏—è':    ['–º–∞–≥–∏—è'],
    '–¥–∏–≤–µ—Ä—Å–∏—è': ['–≤–µ—Ä—Å–∏—è'],
    '–Ω–æ–≤–∞–∫':    ['–Ω–æ–≤–∞—è', '–Ω–æ–≤—ã–π', '–Ω–æ–≤–∞', '–Ω–æ–≤–∞—á'],
    '–ø–µ—Å–∫–æ–≤':   ['–≤–µ—Å–æ–≤', '–ª–µ—Å–∫–æ–π', '–º–µ—à–∫–æ–≤', '–ø–µ—à–∫–æ–º', '–¥–∏—Å–∫–æ–≤', '–ø–æ–∏—Å–∫–æ–≤', '–ø–µ—Å–∫–µ', '–ø–∏–∫–æ–≤', '–∫—É—Å–∫–æ–≤', '–ø–µ—Å–æ–∫'],
    '–º–∞–π–¥–∞–Ω':   ['–∑–∞–¥–∞–Ω', '–Ω–∞–π–¥–µ–Ω'],
    '–±–∞–Ω–¥–∏—Ç':   ['–±—É–¥–∏—Ç', '–±–∞–Ω–∫–∏'],
    '–±–æ–µ–≤–∏–∫':   ['–±–æ—Ä—Ç–∏–∫', '–±–æ–ª—Ç–∏–∫'],
    '—Ä–µ—Ñ–æ—Ä–º–∞':  ['—Ñ–æ—Ä–º–∞'],
    '–±–æ–π–∫–æ—Ç':   ['–º–æ–π–∫–æ–π', '–±–æ–∫–æ–º'],
    '–∑–∞—Ö–≤–∞—Ç—á–∏–∫': ['–∑–∞—Ö–≤–∞—Ç–∏—Ç'],
    '–Ω–∞–≤–∞–ª—å–Ω—ã–π': ['–Ω–∞–ø–æ–ª—å–Ω—ã–π', '–Ω–∞—á–∞–ª—å–Ω–æ–π', '–Ω–∞—á–∞–ª—å–Ω—ã–π'],
    '—á–µ—Ä–Ω–µ–Ω–∫–æ': ['—á–µ—Ä–µ–Ω–∫–∞', '—á–µ—Ä–µ–Ω–∫–∏', '—á–µ—Ä–µ–Ω–∫–æ–º'],
    '—Ç–µ—Ä—Ä–æ—Ä':   ['—Ç–µ—Ä–º–æ—Å', '—Ç–µ—Ä–º–æ'],
    '–±–∞–π–¥–µ–Ω':  ['–Ω–∞–π–¥–µ–Ω'],
    '—Ñ–∞—à–∏–∑–º':  ['–≤–∞—à–∏–º'],
    '–ø—Ä–µ—Å—Ç—É–ø–ª–µ–Ω–∏–µ': ['–ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ'],
    '–º–∞—Å–∫':    ['–º–∞—Å–∫–∞', '–º–∞—Å–∫–∏','–º–∞—Å–∫—É','–º–∞—Å–æ–∫'],
    '–±—Ä–µ–∂–Ω–µ–≤': ['–±–µ—Ä–µ–∂–Ω–µ–µ', '–±–µ—Ä–µ–∂–Ω–æ', '–±—Ä–µ–∂–Ω–æ'],
    '–µ–ª—å—Ü–∏–Ω': ['–¥–µ–ª—å—Ñ–∏–Ω'],
    '–≥–∞–ª–∫–∏–Ω':  ['–ø–∞–ª–∫–∏', '–≥–∞–π–∫–∏', '–≥–∞–ª–∏–Ω–∞'],
    '–¥–∏–≤–µ—Ä—Å–∏—è': ['–¥–æ–≤–µ—Ä–∏—è', '–≤–µ—Ä—Å–∏—è'],
    '–ª–∞–≤—Ä–æ–≤':  ['–∫–æ—Ä–æ–≤', '–ª–∏—Ç—Ä–æ–≤', '–∫–æ–≤—Ä–æ–≤'],
    '—É–±–∏–π—Ü–∞':  ['—É–±–∏–ª–∞'],
    '—Å–º–µ—Ä—Ç–Ω–∏–∫': ['—Å–º–µ—Ä—Ç–Ω—ã–π'],
    '—É–≥–æ–ª–æ–≤–Ω–∏–∫': ['–ø–æ–ª–æ–≤–Ω–∏–∫'],
    '—Å—É–¥–µ–±–Ω–∞—è —Ä–µ—Ñ–æ—Ä–º–∞': ['—É–¥–æ–±–Ω–∞—è —Ñ–æ—Ä–º–∞'],
}


def is_fuzzy_match_with_details(review_text, key_phrases):
    # –ü—Ä–æ—Å—Ç–æ –≤—ã–∑—ã–≤–∞–µ–º –∏—Å—Ö–æ–¥–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –∏–∑ utils —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º
    return base_is_fuzzy_match_with_details(
        review_text,
        key_phrases,
    )


_key_phrases_cache = None


def load_phrases_from(filepath="key_words/keywords_700.py"):
    global _key_phrases_cache
    if _key_phrases_cache is not None:
        return _key_phrases_cache

    if not os.path.isfile(filepath):
        logger.error(f"‚ùå [10] –§–∞–π–ª —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Ñ—Ä–∞–∑–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {filepath}")
        raise FileNotFoundError(f"‚ùå [10] –§–∞–π–ª —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Ñ—Ä–∞–∑–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {filepath}")

    from importlib.util import spec_from_file_location, module_from_spec
    spec = spec_from_file_location("phrases_module", filepath)
    module = module_from_spec(spec)
    spec.loader.exec_module(module)

    if hasattr(module, 'key_phrases'):
        key_phrases = getattr(module, 'key_phrases')
        logger.info("[10] –ù–∞–π–¥–µ–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è 'key_phrases' –≤ –º–æ–¥—É–ª–µ.")
    elif hasattr(module, 'KEY_WORDS_700'):
        key_phrases = getattr(module, 'KEY_WORDS_700')
        logger.info("[10] –ù–∞–π–¥–µ–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è 'KEY_WORDS_700' –≤ –º–æ–¥—É–ª–µ.")
    else:
        vars_in_file = dir(module)
        logger.error(f"‚ùå [10] –í —Ñ–∞–π–ª–µ {filepath} –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è 'key_phrases' –∏–ª–∏ 'KEY_WORDS_700'. –ù–∞–π–¥–µ–Ω—ã: {vars_in_file}")
        raise AttributeError(
            f"‚ùå [10] –í —Ñ–∞–π–ª–µ {filepath} –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è 'key_phrases' –∏–ª–∏ 'KEY_WORDS_700'. –ù–∞–π–¥–µ–Ω—ã: {vars_in_file}"
        )

    _key_phrases_cache = key_phrases
    return key_phrases


@pipeline_step(step_number=10, step_name="–ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–† [700]")
def classifier_700(df, stop_pipeline_flag=False, step_number=10):
    logger.info(f"üß† [{step_number}] –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–† [700]: —Å—Ç–∞—Ä—Ç")

    if stop_pipeline_flag:
        logger.warning(f"üîö [{step_number}] –®–∞–≥ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        return df

    if '–û—Ç–∑—ã–≤' not in df.columns or '–ö–ª–∞—Å—Å' not in df.columns:
        logger.error(f"‚ùå [{step_number}] –ö–æ–ª–æ–Ω–∫–∏ '–û—Ç–∑—ã–≤' –∏–ª–∏ '–ö–ª–∞—Å—Å' –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ")
        return df

    keyword_file_path = KEYWORDS_FILES.get(700)
    if not keyword_file_path or not os.path.exists(keyword_file_path):
        logger.error(f"‚ùå [{step_number}] –§–∞–π–ª —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Ñ—Ä–∞–∑–∞–º–∏ –¥–ª—è –∫–ª–∞—Å—Å–∞ 700 –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω: {keyword_file_path}")
        return df

    try:
        key_phrases = load_phrases_from(keyword_file_path)
        logger.info(f"üìå [{step_number}] –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(key_phrases)} –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑ –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞")
    except Exception as e:
        logger.error(f"‚ùå [{step_number}] –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã: {e}")
        return df

    df_filtered = df[~df['–ö–ª–∞—Å—Å'].apply(has_ignored_class)].copy()

    if '–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ' not in df.columns:
        df['–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ'] = ''

    detected_reviews = []
    count_found = 0

    for idx, row in tqdm(df_filtered.iterrows(), total=len(df_filtered), desc="üß† –ò–Ω—Ñ–µ—Ä–µ–Ω—Å [700]"):
        if stop_pipeline_flag:
            logger.warning(f"üîö [{step_number}] –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è [700] –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º –Ω–∞ —Å—Ç—Ä–æ–∫–µ {idx}")
            break

        review_text = str(row.get('–û—Ç–∑—ã–≤') or '').strip()
        if len(review_text) < 5:
            continue

        try:
            found, matches = is_fuzzy_match_with_details(review_text, key_phrases)

            # –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤—ã–≤–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è, –∏—Å–∫–ª—é—á–∞—è —Ç–µ, —É –∫–æ—Ç–æ—Ä—ã—Ö –Ω–∞–π–¥–µ–Ω–Ω–æ–µ —Å–ª–æ–≤–æ –≤ —Å–ø–∏—Å–∫–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–π –¥–ª—è –∫–ª—é—á–µ–≤–æ–π —Ñ—Ä–∞–∑—ã
            filtered_matches = []
            for orig, corr, source in matches:
                exceptions_for_source = ALLOWED_EXCEPTIONS.get(source, [])
                if corr in exceptions_for_source:
                    # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∏–∑ —Å–ø–∏—Å–∫–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–π (–±–µ–∑ –ª–æ–≥–æ–≤)
                    continue
                else:
                    filtered_matches.append((orig, corr, source))

            if filtered_matches:
                count_found += 1
                df = set_class(df, idx, 700)

                note_text = "; ".join(
                    f"—Ñ—Ä–∞–∑–∞: '{source}', –Ω–∞–π–¥–µ–Ω–æ: '{corr}', –≤ –æ—Ç–∑—ã–≤–µ: '{orig}'"
                    for orig, corr, source in filtered_matches
                )
                old_note = df.at[idx, '–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ'] or ''
                df.at[idx, '–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ'] = f"{old_note}; –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: {note_text}" if old_note else f"–ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: {note_text}"

                detected_reviews.append({
                    "–ù–æ–º–µ—Ä —Å—Ç—Ä–æ–∫–∏": idx,
                    "–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞": review_text,
                    "–î–µ—Ç–∞–ª–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è": note_text,
                    "–ü—Ä–æ–¥–∞–≤–µ—Ü": row.get('–ü—Ä–æ–¥–∞–≤–µ—Ü', ''),
                    "–ë—Ä–µ–Ω–¥": row.get('–ù–∞–∑–≤–∞–Ω–∏–µ –±—Ä–µ–Ω–¥–∞', '')
                })
                logger.info(f"[{step_number}] –ù–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞ —Å—Ç—Ä–æ–∫–µ {idx}. –ö–ª—é—á–µ–≤–∞—è —Ñ—Ä–∞–∑–∞: {note_text}.")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [{step_number}] –û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–æ–∫–µ {idx}: {e}")

    logger.info(f"[{step_number}] –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–æ–∫: {len(df_filtered)}, —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º: {count_found}.")

    if SAVE_STEP_10_RESULT and detected_reviews:
        output_file = os.path.join(PROCESSED_DIR, f"step_{step_number}_classifier_700_{TIMESTAMP}.xlsx")
        try:
            df.to_excel(output_file, index=False)
            logger.info(f"üìå [{step_number}] –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {output_file}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [{step_number}] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç classifier_700: {e}")

        details_df = pd.DataFrame(detected_reviews)
        details_file = os.path.join(PROCESSED_DIR, f"step_{step_number}_classifier_700_details_{TIMESTAMP}.xlsx")
        try:
            details_df.to_excel(details_file, index=False)
            logger.info(f"üìÅ [{step_number}] –ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –ø–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–º –æ—Ç–∑—ã–≤–∞–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {details_file}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [{step_number}] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–µ—Ç–∞–ª–∏ classifier_700: {e}")

        logger.info(f"‚úÖ [{step_number}] –ù–∞–π–¥–µ–Ω–æ {count_found} –æ—Ç–∑—ã–≤–æ–≤ —Å –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º")
    else:
        logger.info(f"‚ö†Ô∏è [{step_number}] –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –æ—Ç–∑—ã–≤–∞ —Å –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º")

    logger.info(f"‚úÖ [{step_number}] –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–† [700]: —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω")
    return df