# src/classifiers/classifier_201.py

import logging
import pandas as pd
import os
import re
import sys
import importlib.util
from tqdm import tqdm

from utils import set_class, class_statistics
from decorators import pipeline_step
from config import logger, SAVE_STEP_8_RESULT, PROCESSED_DIR, TIMESTAMP, FILTERED_OUT_CLASS, CLASS_100, KEYWORDS_FILES

logger = logging.getLogger('pipeline')


def load_keywords_201():
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç KEY_WORDS_201 –∏–∑ —Ñ–∞–π–ª–∞ –ø–æ –ø—É—Ç–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ KEYWORDS_FILES[201].
    """
    file_path = KEYWORDS_FILES.get(201, "key_words/key_words_201.py")
    if not os.path.isfile(file_path):
        logger.error(f"‚ùå –§–∞–π–ª –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
        return []

    module_name = "keywords_201"
    spec = importlib.util.spec_from_file_location(module_name, file_path)

    if spec is None:
        logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –º–æ–¥—É–ª—å –∏–∑ {file_path}")
        return []

    module = importlib.util.module_from_spec(spec)
    sys.modules[module_name] = module

    try:
        spec.loader.exec_module(module)
        if hasattr(module, 'KEY_WORDS_201'):
            key_words = module.KEY_WORDS_201
            # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å —É—á—ë—Ç–æ–º word boundaries –∏ –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–∞
            compiled_patterns = [
                (word, re.compile(r'\b' + re.escape(word) + r'\b', re.IGNORECASE))
                for word in key_words
            ]
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏ —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–æ {len(key_words)} –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑ –¥–ª—è –∫–ª–∞—Å—Å–∞ 201")
            return compiled_patterns
        else:
            logger.warning("‚ùå –í –º–æ–¥—É–ª–µ –Ω–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π KEY_WORDS_201")
            return []
    except Exception as e:
        logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥—É–ª—è: {e}")
        return []


@pipeline_step(step_number=8, step_name="–ü–û–ò–°–ö –ü–û –ö–õ–Æ–ß–ï–í–´–ú –°–õ–û–í–ê–ú [201] - –ë/–£")
def classifier_201(df, stop_pipeline_flag=False, step_number=8):
    """
    –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç –∫–ª–∞—Å—Å [201], –µ—Å–ª–∏ –æ—Ç–∑—ã–≤ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞/—Ñ—Ä–∞–∑—ã,
    —É–∫–∞–∑—ã–≤–∞—é—â–∏–µ –Ω–∞ —Ç–æ, —á—Ç–æ —Ç–æ–≤–∞—Ä –±—ã–ª –≤ —É–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–∏ (–±/—É).
    –ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –æ—Ç–∑—ã–≤—ã —Å –∫–ª–∞—Å—Å–∞–º–∏ [100], [999].
    """

    logger.info(f"üîç [{step_number}] –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–† [201]: —Å—Ç–∞—Ä—Ç ‚Äì –ø–æ–∏—Å–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –±/—É —Ç–æ–≤–∞—Ä–æ–≤")

    if stop_pipeline_flag:
        logger.warning(f"üîö {step_number}] –®–∞–≥ [201] –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        return df

    if df is None or df.empty:
        logger.warning(f"üü° [{step_number}] –í—Ö–æ–¥–Ω–æ–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –ø—É—Å—Ç–æ–π –∏–ª–∏ None")
        return df

    if '–ö–ª–∞—Å—Å' not in df.columns:
        logger.error(f"‚ùå [{step_number}] –í—Ö–æ–¥–Ω–æ–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–æ–ª–æ–Ω–∫—É '–ö–ª–∞—Å—Å'.")
        return df

    if '–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ' not in df.columns:
        df['–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ'] = ''

    def has_ignored_class(classes):
        if isinstance(classes, list):
            return FILTERED_OUT_CLASS in classes or CLASS_100 in classes
        return classes == FILTERED_OUT_CLASS or classes == CLASS_100

    df_to_process = df[~df['–ö–ª–∞—Å—Å'].apply(has_ignored_class)].copy()

    compiled_patterns = load_keywords_201()

    if not compiled_patterns:
        logger.warning(f"‚ö†Ô∏è[{step_number}] –ù–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∞ [201] ‚Üí –ø—Ä–æ–ø—É—Å–∫ —à–∞–≥–∞")
        return df

    found_count = 0
    detected_reviews = []

    for idx, row in tqdm(df_to_process.iterrows(), total=len(df_to_process), desc="üîé –ü–æ–∏—Å–∫ –±/—É —Å–ª–æ–≤ [201]"):
        if stop_pipeline_flag:
            logger.warning(f"üîö [{step_number}] –®–∞–≥ –ø—Ä–µ—Ä–≤–∞–Ω –≤–Ω—É—Ç—Ä–∏ —Ü–∏–∫–ª–∞")
            break

        review_text = str(row.get('–û—Ç–∑—ã–≤', '')).lower()

        found_words_set = set()  # —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø–æ–≤—Ç–æ—Ä–æ–≤

        for word, pattern in compiled_patterns:
            matches = pattern.findall(review_text)
            if matches:
                found_words_set.add(word)

        if found_words_set:
            df = set_class(df, idx, code=201)
            found_count += 1

            old_note = df.at[idx, '–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ']
            found_str = ", ".join(sorted(found_words_set))
            new_note = f"{old_note} | –±/—É –ø—Ä–∏–∑–Ω–∞–∫: '{found_str}'" if old_note else f"–±/—É –ø—Ä–∏–∑–Ω–∞–∫: '{found_str}'"
            df.at[idx, '–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ'] = new_note

            detected_reviews.append({
                '–ù–æ–º–µ—Ä —Å—Ç—Ä–æ–∫–∏': idx,
                '–û—Ç–∑—ã–≤': df.at[idx, '–û—Ç–∑—ã–≤'],
                '–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è': str(df.at[idx, '–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è']) if '–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è' in df.columns else '',
                '–ü—Ä–æ–¥–∞–≤–µ—Ü': df.at[idx, '–ü—Ä–æ–¥–∞–≤–µ—Ü'] if '–ü—Ä–æ–¥–∞–≤–µ—Ü' in df.columns else '',
                '–ù–∞–π–¥–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –±/—É': found_str
            })

    logger.info(f"üìå [{step_number}] –ù–∞–π–¥–µ–Ω–æ {found_count} –æ—Ç–∑—ã–≤–æ–≤ —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –±/—É —Ç–æ–≤–∞—Ä–æ–≤. –ü—Ä–æ—Å—Ç–∞–≤–ª–µ–Ω –∫–ª–∞—Å—Å [201]")

    if SAVE_STEP_8_RESULT and found_count > 0:
        output_file = os.path.join(PROCESSED_DIR, f"step_{step_number}_used_items_{TIMESTAMP}.xlsx")

        df_to_save = df.copy()
        if '–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è' in df_to_save.columns:
            df_to_save['–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è'] = df_to_save['–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è'].apply(
                lambda x: x.replace(tzinfo=None) if pd.notna(x) and hasattr(x, 'tz') else x
            )

        try:
            df_to_save.to_excel(output_file, index=False)
            logger.info(f"üìå [{step_number}] –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {output_file}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [{step_number}] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç classifier_201: {e}")

        details_file = os.path.join(PROCESSED_DIR, f"step_8_used_items_details_{TIMESTAMP}.xlsx")
        try:
            pd.DataFrame(detected_reviews).to_excel(details_file, index=False)
            logger.info(f"üìÅ [{step_number}] –î–µ—Ç–∞–ª–∏ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {details_file}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è[{step_number}] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–µ—Ç–∞–ª–∏ classifier_201: {e}")

        try:
            stats = class_statistics(df)
            logger.info(f"\nüìä [{step_number}] –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤—Å–µ–º –∫–ª–∞—Å—Å–∞–º –ø–æ—Å–ª–µ classifier_201:\n")
            logger.info(stats.to_string(index=False))
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [{step_number}] –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–≤–µ—Å—Ç–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É ‚Äî {e}")

    return df
