# src/classifiers/classifier_701.py

import os
import re
import torch
import numpy as np
import pandas as pd
import pymorphy3
import pickle
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
from tqdm import tqdm
import traceback

from decorators import pipeline_step
from config import (
    logger,
    WORD2IDX_701_PATH,
    EMBED_MATRIX_701_PATH,
    MODEL_701_WEIGHTS_PATH,
    SAVE_STEP_11_RESULT,
    PROCESSED_DIR,
    TIMESTAMP
)
from utils import set_class, class_statistics


MAX_LENGTH = 120
EMBEDDING_DIM = 300
HIDDEN_SIZE = 128
NUM_CLASSES = 2
CNN_KERNELS = (3, 4, 5)
CNN_FILTERS = 64
DROPOUT = 0.4


morph = pymorphy3.MorphAnalyzer()
pos_map = {
    'NOUN': 'NOUN', 'VERB': 'VERB', 'INFN': 'VERB', 'ADJF': 'ADJ', 'ADJS': 'ADJ',
    'COMP': 'ADJ', 'PRTF': 'ADJ', 'PRTS': 'ADJ', 'GRND': 'VERB', 'NUMR': 'NUM',
    'ADVB': 'ADV', 'NPRO': 'PRON', 'PRED': 'ADV', 'PREP': 'ADP', 'CONJ': 'CONJ',
    'PRCL': 'PART', 'INTJ': 'INTJ'
}


def get_pos_tag(word):
    parse = morph.parse(word)[0]
    pos = parse.tag.POS
    return pos_map.get(pos, 'X')


def process_token(token):
    parse = morph.parse(token)[0]
    lemma = parse.normal_form
    pos = get_pos_tag(token)
    return f"{lemma}_{pos}"


def preproc_text(text):
    tokens = re.findall(r"\w+", text.lower())
    return " ".join(process_token(token) for token in tokens)


def text_to_sequence(text, word2idx, max_length=MAX_LENGTH):
    tokens = text.split()
    indices = [word2idx.get(t, 1) for t in tokens]  # 1 ‚Äî <UNK>
    if len(indices) < max_length:
        indices += [0] * (max_length - len(indices))
    else:
        indices = indices[:max_length]
    return indices


class ReviewDataset(Dataset):
    def __init__(self, texts, word2idx):
        self.sequences = [text_to_sequence(t, word2idx) for t in texts]

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        return torch.tensor(self.sequences[idx], dtype=torch.long)


class CNN_BiLSTM_Attention(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_size, num_classes,
                 kernel_sizes=(3, 4, 5), num_filters=64, dropout_rate=0.4, embed_matrix=None):
        super().__init__()
        if embed_matrix is not None:
            self.embedding = nn.Embedding.from_pretrained(
                torch.FloatTensor(embed_matrix), freeze=False, padding_idx=0)
        else:
            self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.convs = nn.ModuleList([nn.Conv1d(embedding_dim, num_filters, k) for k in kernel_sizes])
        self.lstm = nn.LSTM(num_filters * len(kernel_sizes), hidden_size,
                            batch_first=True, bidirectional=True)
        self.dropout = nn.Dropout(dropout_rate)
        self.attn_fc = nn.Linear(hidden_size * 2, 1)
        self.fc = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_size, num_classes)
        )

    def forward(self, x):
        emb = self.embedding(x).transpose(1, 2)
        convs = [torch.relu(conv(emb)) for conv in self.convs]
        min_len = min(conv.shape[2] for conv in convs)
        convs = [conv[:, :, :min_len] for conv in convs]
        cnn_out = torch.cat(convs, dim=1).transpose(1, 2)
        lstm_out, _ = self.lstm(cnn_out)
        attn_weights = torch.softmax(self.attn_fc(lstm_out), dim=1)
        attended = torch.sum(attn_weights * lstm_out, dim=1)
        out = self.dropout(attended)
        return self.fc(out)


def has_ignored_class(classes):
    if isinstance(classes, list):
        return 999 in classes or 100 in classes
    return classes == 999 or classes == 100


@pipeline_step(step_number=11, step_name="–ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–† [701]")
def classifier_701(df, stop_pipeline_flag=False, threshold=0.9, step_number=11):
    """
    –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å –ø–æ—Ä–æ–≥–æ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞.

    :param df: DataFrame —Å –∫–æ–ª–æ–Ω–∫–æ–π '–û—Ç–∑—ã–≤'
    :param stop_pipeline_flag: —Ñ–ª–∞–≥ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ pipeline
    :param threshold: –ø–æ—Ä–æ–≥ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤ –∫–ª–∞—Å—Å 701
    :param step_number: –Ω–æ–º–µ—Ä —à–∞–≥–∞ –ø–∞–π–ø–ª–∞–π–Ω–∞
    :return: DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º –∫–ª–∞—Å—Å–æ–º 701 –∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏
    """
    logger.info(f"üß† [{step_number}] –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–† [701]: —Å—Ç–∞—Ä—Ç (–ø–æ—Ä–æ–≥={threshold})")
    try:
        if stop_pipeline_flag:
            logger.warning(f"üîö [{step_number}] –®–∞–≥ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            return df

        if '–û—Ç–∑—ã–≤' not in df.columns or '–ö–ª–∞—Å—Å' not in df.columns:
            logger.error(f"‚ùå [{step_number}] –ö–æ–ª–æ–Ω–∫–∏ '–û—Ç–∑—ã–≤' –∏–ª–∏ '–ö–ª–∞—Å—Å' –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ")
            return df

        # –§–∏–ª—å—Ç—Ä—É–µ–º –æ—Ç–∑—ã–≤—ã —Å –∫–ª–∞—Å—Å–∞–º–∏ 999 –∏ 100
        df_filtered = df[~df['–ö–ª–∞—Å—Å'].apply(has_ignored_class)].copy()

        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        df_filtered['text_processed'] = df_filtered['–û—Ç–∑—ã–≤'].astype(str).apply(preproc_text)

        with open(WORD2IDX_701_PATH, "rb") as f:
            word2idx = pickle.load(f)
        embed_matrix = np.load(EMBED_MATRIX_701_PATH)

        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        model = CNN_BiLSTM_Attention(
            vocab_size=len(word2idx),
            embedding_dim=EMBEDDING_DIM,
            hidden_size=HIDDEN_SIZE,
            num_classes=NUM_CLASSES,
            kernel_sizes=CNN_KERNELS,
            num_filters=CNN_FILTERS,
            dropout_rate=DROPOUT,
            embed_matrix=embed_matrix
        )
        model.load_state_dict(torch.load(MODEL_701_WEIGHTS_PATH, map_location=device))
        model.to(device)
        model.eval()

        dataset = ReviewDataset(df_filtered['text_processed'].tolist(), word2idx)
        dataloader = DataLoader(dataset, batch_size=32, shuffle=False)

        preds = []
        probs_701 = []

        for batch in tqdm(dataloader, desc=f"üß† [{step_number}] –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è [701]"):
            if stop_pipeline_flag:
                logger.warning(f"üîö [{step_number}] –®–∞–≥ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º –≤–Ω—É—Ç—Ä–∏ —Ü–∏–∫–ª–∞")
                break
            batch = batch.to(device)
            logits = model(batch)
            prob = torch.softmax(logits, dim=1)
            prob_class_701 = prob[:, 1]
            pred = (prob_class_701 > threshold).cpu().tolist()
            preds.extend(pred)
            probs_701.extend(prob_class_701.cpu().tolist())

        # –ü—Ä–æ—Å—Ç–∞–≤–ª—è–µ–º –∫–ª–∞—Å—Å 701 –¥–ª—è —Ç–µ—Ö, —É –∫–æ–≥–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞
        for idx, pred in zip(df_filtered.index, preds):
            if pred:
                df = set_class(df, idx, 701)

        # –î–æ–±–∞–≤–ª—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–∞ 701 –≤ –æ—Å–Ω–æ–≤–Ω–æ–π df
        df.loc[df_filtered.index, 'prob_701'] = pd.Series(probs_701, index=df_filtered.index)

        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Å—Ç–æ–ª–±–µ—Ü
        df_filtered.drop(columns=['text_processed'], inplace=True)
        if 'text_processed' in df.columns:
            df.drop(columns=['text_processed'], inplace=True)

        if SAVE_STEP_11_RESULT:
            try:
                output_file = os.path.join(PROCESSED_DIR, f"step_{step_number}_classifier_701_{TIMESTAMP}.xlsx")
                df.to_excel(output_file, index=False)
                logger.info(f"üìå [{step_number}] –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {output_file}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è [{step_number}] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã classifier_701: {e}")

        try:
            stats_df = class_statistics(df)
            logger.info(f"\nüìù [{step_number}] –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–ª–∞—Å—Å–æ–≤ –ø–æ—Å–ª–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ 701:\n%s", stats_df.to_string(index=False))
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [{step_number}] –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–≤–µ—Å—Ç–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ—Å–ª–µ classifier_701: {e}")

        logger.info(f"‚úÖ [{step_number}] –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–† [701]: —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω")
        return df

    except Exception as ex:
        logger.error(f"‚ùå [{step_number}] –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–† [701]: –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ ‚Äî {ex}")
        logger.error(traceback.format_exc())
        return None
