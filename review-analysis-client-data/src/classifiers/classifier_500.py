# src/classifiers/classifier_500.py

import logging
import pandas as pd
import os
import re
import sys
import importlib.util
from tqdm import tqdm

from utils import set_class, class_statistics
from decorators import pipeline_step
from config import logger, SAVE_STEP_6_RESULT, PROCESSED_DIR, TIMESTAMP, FILTERED_OUT_CLASS, CLASS_100

logger = logging.getLogger('pipeline')


def load_keywords(file_path="key_words/keywords_500.py"):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç KEY_WORDS_500 –∏–∑ –≤–Ω–µ—à–Ω–µ–≥–æ —Ñ–∞–π–ª–∞.
    """
    if not os.path.isfile(file_path):
        logger.error(f"‚ùå –§–∞–π–ª –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
        return []

    module_name = "keywords_500"
    spec = importlib.util.spec_from_file_location(module_name, file_path)

    if spec is None:
        logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –º–æ–¥—É–ª—å –∏–∑ {file_path}")
        return []

    module = importlib.util.module_from_spec(spec)
    sys.modules[module_name] = module

    try:
        spec.loader.exec_module(module)
        if hasattr(module, 'KEY_WORDS_500'):
            key_words = module.KEY_WORDS_500
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(key_words)} –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –Ω–µ—Ü–µ–Ω–∑—É—Ä–Ω–æ–π –ª–µ–∫—Å–∏–∫–∏")
            return key_words
        else:
            logger.warning("‚ùå –í –º–æ–¥—É–ª–µ –Ω–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π KEY_WORDS_500")
            return []
    except Exception as e:
        logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥—É–ª—è: {e}")
        return []


@pipeline_step(step_number=6, step_name="–ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –Ω–µ—Ü–µ–Ω–∑—É—Ä–Ω–æ–π –ª–µ–∫—Å–∏–∫–∏")
def classifier_500(df, stop_pipeline_flag=False, step_number=6):
    """
    –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç –∫–ª–∞—Å—Å [500], –µ—Å–ª–∏ –æ—Ç–∑—ã–≤ —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–¥–Ω–æ –∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤.
    –ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –æ—Ç–∑—ã–≤—ã —Å –∫–ª–∞—Å—Å–∞–º–∏ [100] –∏ [999].
    """

    if stop_pipeline_flag:
        logger.warning(f"üîö [{step_number}] –®–∞–≥ [500] –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        return df

    if df is None or df.empty:
        logger.warning(f"üü° [{step_number}] –í—Ö–æ–¥–Ω–æ–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –ø—É—Å—Ç–æ–π –∏–ª–∏ None")
        return df

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–æ–ª–æ–Ω–æ–∫
    if '–ö–ª–∞—Å—Å' not in df.columns:
        logger.error(f"‚ùå [{step_number}] –í—Ö–æ–¥–Ω–æ–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–æ–ª–æ–Ω–∫—É '–ö–ª–∞—Å—Å'.")
        return df

    if '–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ' not in df.columns:
        df['–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ'] = ''

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –æ—Ç–∑—ã–≤–æ–≤ —Å –∫–ª–∞—Å—Å–∞–º–∏ 999 –∏ 100
    def has_ignored_class(classes):
        if isinstance(classes, list):
            return FILTERED_OUT_CLASS in classes or CLASS_100 in classes
        return classes == FILTERED_OUT_CLASS or classes == CLASS_100

    df_to_process = df[~df['–ö–ª–∞—Å—Å'].apply(has_ignored_class)].copy()

    from config import KEYWORDS_FILES
    keywords_path = KEYWORDS_FILES.get(500, "key_words/keywords_500.py")
    key_words = load_keywords(keywords_path)

    if not key_words:
        logger.warning(f"‚ö†Ô∏è [{step_number}] –ù–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ ‚Üí –ø—Ä–æ–ø—É—Å–∫ —à–∞–≥–∞ [6]")
        return df

    found_count = 0
    detected_reviews = []

    for idx, row in tqdm(df_to_process.iterrows(), total=len(df_to_process), desc=f"üîé [{step_number}] –ü–æ–∏—Å–∫ —Å–ª–æ–≤ [500]"):
        if stop_pipeline_flag:
            logger.warning(f"üîö [{step_number}] –®–∞–≥ –ø—Ä–µ—Ä–≤–∞–Ω –≤–Ω—É—Ç—Ä–∏ —Ü–∏–∫–ª–∞")
            break

        review_text = str(row['–û—Ç–∑—ã–≤']).lower()
        for word in key_words:
            if len(word) < 3:
                continue
            if re.search(rf'\b{re.escape(word)}\b', review_text, re.IGNORECASE):
                df = set_class(df, idx, code=500)

                old_note = df.at[idx, '–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ']
                new_note = f"{old_note} | –Ω–∞–π–¥–µ–Ω–æ —Å–ª–æ–≤–æ '{word}'" if old_note else f"–Ω–∞–π–¥–µ–Ω–æ —Å–ª–æ–≤–æ '{word}'"
                df.at[idx, '–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ'] = new_note

                found_count += 1

                detected_reviews.append({
                    '–ù–æ–º–µ—Ä —Å—Ç—Ä–æ–∫–∏': idx,
                    '–û—Ç–∑—ã–≤': df.at[idx, '–û—Ç–∑—ã–≤'],
                    '–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è': str(df.at[idx, '–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è']) if '–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è' in df.columns else '',
                    '–ü—Ä–æ–¥–∞–≤–µ—Ü': df.at[idx, '–ü—Ä–æ–¥–∞–≤–µ—Ü'] if '–ü—Ä–æ–¥–∞–≤–µ—Ü' in df.columns else '',
                    '–ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ': word
                })
                break  # –ü—Ä–µ–∫—Ä–∞—â–∞–µ–º –ø–æ–∏—Å–∫ –ø–æ —Å–ª–æ–≤–∞–º –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –æ—Ç–∑—ã–≤–∞ –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è

    logger.info(f"üìå [{step_number}] –ù–∞–π–¥–µ–Ω–æ {found_count} –æ—Ç–∑—ã–≤–æ–≤ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏. –ü—Ä–æ—Å—Ç–∞–≤–ª–µ–Ω –∫–ª–∞—Å—Å [500]")

    if SAVE_STEP_6_RESULT and found_count > 0:
        output_file = os.path.join(PROCESSED_DIR, f"step_{step_number}_keywords_{TIMESTAMP}.xlsx")

        # –£–±–∏—Ä–∞–µ–º —Ç–∞–π–º–∑–æ–Ω—ã –∏–∑ '–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è'
        df_to_save = df.copy()

        if '–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è' in df_to_save.columns:
            df_to_save['–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è'] = df_to_save['–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è'].apply(
                lambda x: x.replace(tzinfo=None) if pd.notna(x) and hasattr(x, 'tz') else x
            )

        try:
            df_to_save.to_excel(output_file, index=False)
            logger.info(f"üìå [{step_number}] –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {output_file}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [{step_number}] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç classifier_500: {e}")

        details_file = os.path.join(PROCESSED_DIR, f"step_6_keywords_details_{TIMESTAMP}.xlsx")
        try:
            pd.DataFrame(detected_reviews).to_excel(details_file, index=False)
            logger.info(f"üìÅ [{step_number}] –î–µ—Ç–∞–ª–∏ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {details_file}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [{step_number}] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–µ—Ç–∞–ª–∏ classifier_500: {e}")

        try:
            stats = class_statistics(df)
            logger.info(f"\nüìä [{step_number}] –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤—Å–µ–º –∫–ª–∞—Å—Å–∞–º –ø–æ—Å–ª–µ classifier_500:\n")
            logger.info(stats.to_string(index=False))
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [{step_number}] –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–≤–µ—Å—Ç–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É ‚Äî {e}")

    return df
