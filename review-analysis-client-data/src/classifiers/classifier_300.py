# src/classifiers/classifier_300.py

import logging
import os
import pandas as pd
from tqdm import tqdm

from utils import set_class, class_statistics
from decorators import pipeline_step
from config import logger, SAVE_STEP_5_RESULT, PROCESSED_DIR, TIMESTAMP, FILTERED_OUT_CLASS, CLASS_100


@pipeline_step(step_number=5, step_name="–ü–û–ò–°–ö –î–£–ë–õ–ò–ö–ê–¢–û–í ‚Üí [300]")
def classifier_300(df, stop_pipeline_flag=False, step_number=5):
    """
    –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç –∫–ª–∞—Å—Å [300], –µ—Å–ª–∏ –æ—Ç–∑—ã–≤ —è–≤–ª—è–µ—Ç—Å—è –¥—É–±–ª–∏–∫–∞—Ç–æ–º –≤–Ω—É—Ç—Ä–∏ –¥–Ω—è.
    –ü—Ä–∏—á–∏–Ω–∞ –ø–æ–º–µ—Ç–∫–∏: –¥—É–±–ª—å –∫ —Å—Ç—Ä–æ–∫–µ + –≤—Ä–µ–º—è –æ—Ä–∏–≥–∏–Ω–∞–ª–∞
    """

    if stop_pipeline_flag:
        logger.warning(f"üîö [{step_number}] –®–∞–≥ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        return df

    if df is None or df.empty:
        logger.warning(f"üü° [{step_number}] –í—Ö–æ–¥–Ω–æ–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –ø—É—Å—Ç–æ–π –∏–ª–∏ None")
        return df

    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –∫–æ–ª–æ–Ω–∫–∞ '–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ' —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    if '–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ' not in df.columns:
        df['–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ'] = ''

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º—ã—Ö –∫–ª–∞—Å—Å–æ–≤
    def has_ignored_class(classes):
        if isinstance(classes, list):
            return FILTERED_OUT_CLASS in classes or CLASS_100 in classes
        return classes == FILTERED_OUT_CLASS or classes == CLASS_100

    # --- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ ---
    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è –ø—Ä—è–º–æ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π df, —Ç.–∫. –ø–æ –Ω–µ–º—É –∏ –±—É–¥–µ–º –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å
    df['–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è'] = pd.to_datetime(df['–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è'], errors='coerce')
    df['date_only'] = df['–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è'].dt.date
    df['group_key'] = df['–û—Ç–∑—ã–≤'].astype(str) + ' | ' + df['–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞'].astype(str)

    group_cols = ['date_only', 'group_key', '–ü—Ä–æ–¥–∞–≤–µ—Ü']
    duplicates_count = 0
    detected_duplicates = []

    # –ò—Ç–µ—Ä–∏—Ä—É–µ–º—Å—è –ø–æ –≥—Ä—É–ø–ø–∞–º –æ—Å–Ω–æ–≤–Ω–æ–≥–æ DataFrame
    for group_keys, group in tqdm(df.groupby(group_cols), total=df[group_cols].nunique(dropna=False).prod(), desc="üîç –ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"):
        if stop_pipeline_flag:
            logger.warning(f"üîö [{step_number}] –®–∞–≥ –ø—Ä–µ—Ä–≤–∞–Ω –≤–Ω—É—Ç—Ä–∏ —Ü–∏–∫–ª–∞")
            break

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–µ–∫—É—â—É—é –≥—Ä—É–ø–ø—É, –∏—Å–∫–ª—é—á–∞—è –æ—Ç–∑—ã–≤—ã —Å —É–∂–µ –ø—Ä–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º—ã–º–∏ –∫–ª–∞—Å—Å–∞–º–∏
        # (—ç—Ç–æ –≤–∞–∂–Ω–æ, —Ç.–∫. df['–ö–ª–∞—Å—Å'] –º–æ–∂–µ—Ç –º–µ–Ω—è—Ç—å—Å—è –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞—Ö –∏–ª–∏ –Ω–∞ —ç—Ç–æ–º —à–∞–≥–µ)
        filtered_group = group[~group['–ö–ª–∞—Å—Å'].apply(has_ignored_class)]

        if len(filtered_group) > 1:
            # –û—Ç—Å–æ—Ä—Ç–∏—Ä—É–µ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—É—é –≥—Ä—É–ø–ø—É –ø–æ –¥–∞—Ç–µ —Å–æ–∑–¥–∞–Ω–∏—è
            sorted_filtered_group = filtered_group.sort_values(by='–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è')
            indices_sorted = sorted_filtered_group.index.tolist()

            first_idx = indices_sorted[0]
            duplicate_indices = indices_sorted[1:]  # –í—Å–µ –∏–Ω–¥–µ–∫—Å—ã, –∫—Ä–æ–º–µ –ø–µ—Ä–≤–æ–≥–æ (–æ—Ä–∏–≥–∏–Ω–∞–ª–∞)

            for idx in duplicate_indices:
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è, –Ω–æ —É–∂–µ –∏–∑–±—ã—Ç–æ—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞, –µ—Å–ª–∏ has_ignored_class –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫ filtered_group
                # current_class = df.at[idx, '–ö–ª–∞—Å—Å']
                # if isinstance(current_class, list) and (FILTERED_OUT_CLASS in current_class or CLASS_100 in current_class):
                #     continue

                df = set_class(df, idx, code=300)
                original_time = str(df.at[first_idx, '–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è'])

                old_note = df.at[idx, '–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ']
                new_duplicate_note = f"–¥—É–±–ª—å –∫ —Å—Ç—Ä–æ–∫–µ {first_idx}, –≤—Ä–µ–º—è –æ—Ä–∏–≥–∏–Ω–∞–ª–∞ {original_time}"
                df.at[idx, '–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ'] = f"{old_note} | {new_duplicate_note}" if old_note else new_duplicate_note

                duplicates_count += 1

                detected_duplicates.append({
                    '–ù–æ–º–µ—Ä —Å—Ç—Ä–æ–∫–∏': idx,
                    '–û—Ç–∑—ã–≤': df.at[idx, '–û—Ç–∑—ã–≤'],
                    '–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è': str(df.at[idx, '–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è']),
                    '–ü—Ä–æ–¥–∞–≤–µ—Ü': df.at[idx, '–ü—Ä–æ–¥–∞–≤–µ—Ü'],
                    '–î—É–±–ª—å –∫ —Å—Ç—Ä–æ–∫–µ': first_idx,
                    '–í—Ä–µ–º—è –æ—Ä–∏–≥–∏–Ω–∞–ª–∞': original_time
                })

    # --- –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ ---
    df.drop(columns=['date_only', 'group_key'], inplace=True, errors='ignore')

    # --- –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã ---
    logger.info(f"‚úÖ [{step_number}] –ù–∞–π–¥–µ–Ω–æ {duplicates_count} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤. –ü—Ä–æ—Å—Ç–∞–≤–ª–µ–Ω –∫–ª–∞—Å—Å [300]")

    # --- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ ---
    if SAVE_STEP_5_RESULT and duplicates_count > 0:
        output_file = os.path.join(PROCESSED_DIR, f"step_5_duplicates_{TIMESTAMP}.xlsx")

        # --- –£–±–∏—Ä–∞–µ–º —Ç–∞–π–º–∑–æ–Ω—ã –ø–µ—Ä–µ–¥ –∑–∞–ø–∏—Å—å—é –≤ Excel ---
        df_to_save = df.copy()
        if '–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è' in df_to_save.columns:
            df_to_save['–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è'] = df_to_save['–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è'].apply(
                lambda x: x.replace(tzinfo=None) if pd.notna(x) and hasattr(x, 'tz') else x
            )

        try:
            df_to_save.to_excel(output_file, index=False)
            logger.info(f"üìå [{step_number}] –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {output_file}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [{step_number}] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç classifier_300: {e}")

        # --- –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ ---
        details_file = os.path.join(PROCESSED_DIR, f"step_{step_number}_duplicates_details_{TIMESTAMP}.xlsx")
        try:
            pd.DataFrame(detected_duplicates).to_excel(details_file, index=False)
            logger.info(f"üìÅ [{step_number}] –î–µ—Ç–∞–ª–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {details_file}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [{step_number}] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–µ—Ç–∞–ª–∏ classifier_300: {e}")

        # --- –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É ---
        try:
            stats = class_statistics(df)
            logger.info(f"\nüìä [{step_number}] –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤—Å–µ–º –∫–ª–∞—Å—Å–∞–º –ø–æ—Å–ª–µ classifier_300:\n")
            logger.info(stats.to_string(index=False))
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [{step_number}] –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–≤–µ—Å—Ç–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É ‚Äî {e}")

    return df
