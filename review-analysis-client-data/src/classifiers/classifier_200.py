# src/classifiers/classifier_200.py

import os
import re
import pandas as pd
from tqdm import tqdm

from decorators import pipeline_step
from utils import set_class, class_statistics
from config import logger, SAVE_STEP_4_RESULT, PROCESSED_DIR, TIMESTAMP, FILTERED_OUT_CLASS, CLASS_100

# –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —Å—Å—ã–ª–æ–∫, —Ç–µ–ª–µ–≥—Ä–∞–º, —Å–æ—Ü—Å–µ—Ç–µ–π
URL_PATTERNS = [
    r"https?://(?:www\.)?\S+",
    r"www\.\S+",
    r"\S+\.(?:com|ru|net|org|info|biz|gov|edu|io|co|me)(?:/\S*)?",
]

SOCIAL_MEDIA_PATTERNS = [
    r"t\.me/\S+",
    r"telegram\.me/\S+",
    r"vk\.com/\S+",
    r"facebook\.com/\S+",
    r"instagram\.com/\S+",
    r"ok\.ru/\S+",
    r"twitter\.com/\S+",
    r"youtube\.com/\S+",
]

KEYWORD_PATTERNS = [
    r"\b–∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Ä–µ—Å—É—Ä—Å\b",
]

# –ö–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ (—Å –Ω–µ–∫–∞–ø—Ç—É—Ä–∏—Ä—É—é—â–∏–º–∏ –≥—Ä—É–ø–ø–∞–º–∏)
spam_regex = [
    # üéØ –Ø–≤–Ω—ã–µ —Ä–µ–∫–ª–∞–º–Ω—ã–µ –∏ –ø—Ä–æ–º–æ-—Ñ—Ä–∞–∑—ã
    r"\b—Å–∫–∏–¥–∫–∏ –¥–æ\b", r"\b–∫—É–ø–æ–Ω –Ω–∞ —Å–∫–∏–¥–∫—É\b", r"\b—Å–∫–∏–¥–æ—á–Ω—ã–π –∫—É–ø–æ–Ω\b", r"\b—Å—É–ø–µ—Ä —Ü–µ–Ω–∞\b", r"\b–∞–∫—Ü–∏—è\b",
    r"\b—Ä–∞—Å–ø—Ä–æ–¥–∞–∂–∞\b", r"\b—Ä–∞—Å–ø—Ä–æ–¥–∞–∂–∞ —Ç–æ–≤–∞—Ä–æ–≤\b", r"\b—É—Å–ø–µ–π –∫—É–ø–∏—Ç—å\b", r"\b–≤—ã–≥–æ–¥–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ\b",
    r"\b–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ\b", r"\b–ø–æ–ª—É—á–∏—Ç—å –ø–æ–¥–∞—Ä–æ–∫\b", r"\b–ø–æ–ª—É—á–∏—Ç—å –ø–æ–¥–∞—Ä–∫–∏\b",
    r"\b–ø–æ–ª—É—á–∏—Ç—å –±–µ—Å–ø–ª–∞—Ç–Ω–æ\b", r"\b–ø—Ä–∏–∑\b", r"\b–≤—ã–∏–≥—Ä–∞–π\b", r"\b–∫–æ–Ω–∫—É—Ä—Å\b", r"\b—Ä–æ–∑—ã–≥—Ä—ã—à\b",

    # üîó –°—Å—ã–ª–∫–∏ –∏ –ø–µ—Ä–µ—Ö–æ–¥—ã
    r"\b–ø–æ –ø—Ä–æ–º–æ–∫–æ–¥—É\b", r"\b–ø–µ—Ä–µ—Ö–æ–¥–∏ –ø–æ —Å—Å—ã–ª–∫–µ\b", r"\b–ø–µ—Ä–µ—Ö–æ–¥–∏—Ç–µ –ø–æ —Å—Å—ã–ª–∫–µ\b",
    r"\b—Å—Å—ã–ª–∫–∞ –≤ –ø—Ä–æ—Ñ–∏–ª–µ\b", r"\b—Å—Å—ã–ª–∫–∞ –Ω–∏–∂–µ\b", r"\b–Ω–∞ –Ω–∞—à–µ–º —Å–∞–π—Ç–µ\b", r"\b–∏—â–∏—Ç–µ –≤ –≥—É–≥–ª–µ\b", r"\b–∏—â–∏—Ç–µ –≤ —è–Ω–¥–µ–∫—Å–µ\b",
    r"(?:wa\.me|t\.me|–≤–æ—Ç—Å–∞–ø|\b—Ç–µ–ª–µ–≥–∞\b|\b–≤–∞–π–±–µ—Ä\b)", r"(?:—Ä–µ—Ñ–µ—Ä–∞–ª—å–Ω–∞—è|–ø–∞—Ä—Ç–Ω—ë—Ä—Å–∫–∞—è)\s+—Å—Å—ã–ª–∫–∞",
    r"(?:–∑–∞—Ä–∞–±–æ—Ç–∞–π|–ø–æ–ª—É—á–∏)\s+(?:–±–æ–Ω—É—Å|—Å–∫–∏–¥–∫—É|–∫—ç—à–±—ç–∫)\s+–ø–æ\s+—Å—Å—ã–ª–∫–µ",

    # üí¨ –ü—Ä–∏–∑—ã–≤—ã –∫ —Å–≤—è–∑—è–º / –∫–æ–Ω—Ç–∞–∫—Ç–∞–º
    r"\b–ø–∏—à–∏ –≤ –ª–∏—á–∫—É\b", r"\b–Ω–∞–ø–∏—à–∏ –≤ –ª–∏—á–∫—É\b", r"\b–Ω–∞–ø–∏—Å–∞—Ç—å –≤ –ª–∏—á–∫—É\b", r"\b–ø–∏—à–∏—Ç–µ –≤ –ª—Å\b", r"\b–≤ –¥–∏—Ä–µ–∫—Ç\b",
    r"\b–ª–∏—á–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è\b", r"(?:–ø–∏—à–∏—Ç–µ|—Å–≤—è–∂–∏—Ç–µ—Å—å)\s+–≤\s+(?:–ª–∏—á–∫—É|–¥–∏—Ä–µ–∫—Ç|–ª–∏—á–Ω—ã–µ\s+—Å–æ–æ–±—â–µ–Ω–∏—è)",
    r"\b–º—ã –≤–∞–º –ø–µ—Ä–µ–∑–≤–æ–Ω–∏–º\b", r"\b–æ—Å—Ç–∞–≤—å—Ç–µ –Ω–æ–º–µ—Ä\b", r"\b—Å–≤—è–∂–µ–º—Å—è —Å –≤–∞–º–∏\b",
    r"(?:–≤—Å—Ç—É–ø–∞–π|–≤—Å—Ç—É–ø–∏—Ç–µ)\s+–≤\s+(?:—á–∞—Ç|–≥—Ä—É–ø–ø—É)",

    # üì± –ú–µ—Å—Å–µ–Ω–¥–∂–µ—Ä—ã –∏ —Å–æ—Ü—Å–µ—Ç–∏
    r"\b—Ç–µ–ª–µ–≥—Ä–∞–º\b", r"\btg\b", r"\btelegram\b", r"\b–≤–∞–π–±–µ—Ä\b", r"\bviber\b", r"\b–≤–∞—Ç—Å–∞–ø\b", r"\bwhatsapp\b",
]

ALL_PATTERNS = URL_PATTERNS + SOCIAL_MEDIA_PATTERNS + KEYWORD_PATTERNS
COMPILED_PATTERN = re.compile("|".join(ALL_PATTERNS), flags=re.IGNORECASE)
SPAM_PATTERN = re.compile("|".join(spam_regex), flags=re.IGNORECASE)


def extract_matches(findall_result):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç re.findall, –≤–æ–∑–≤—Ä–∞—â–∞—è —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫.
    –ï—Å–ª–∏ —à–∞–±–ª–æ–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç –≥—Ä—É–ø–ø—ã, findall –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—Ä—Ç–µ–∂–∏.
    –ë–µ—Ä—ë—Ç –ø–µ—Ä–≤—ã–π –Ω–µ–ø—É—Å—Ç–æ–π —ç–ª–µ–º–µ–Ω—Ç –∏–∑ –∫–∞–∂–¥–æ–≥–æ –∫–æ—Ä—Ç–µ–∂–∞.
    """
    result = []
    for item in findall_result:
        if isinstance(item, tuple):
            for subitem in item:
                if subitem:
                    result.append(subitem)
                    break
        elif isinstance(item, str):
            result.append(item)
    return result


@pipeline_step(step_number=4, step_name="–ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–† [200] ‚Äî –ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ –∏ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑")
def classifier_200(df, stop_pipeline_flag=False, step_number=4):
    logger.info(f"üîç [{step_number}] –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–† [200]: —Å—Ç–∞—Ä—Ç ‚Äì –ø–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ –∏ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑")

    if stop_pipeline_flag:
        logger.warning(f"üîö [{step_number}] –®–∞–≥ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        return df

    if df is None or df.empty:
        logger.warning(f"üü° [{step_number}] –í—Ö–æ–¥–Ω–æ–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –ø—É—Å—Ç–æ–π –∏–ª–∏ None")
        return df

    if '–ö–ª–∞—Å—Å' not in df.columns:
        logger.error(f"‚ùå [{step_number}] –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ '–ö–ª–∞—Å—Å' –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ")
        return df

    def has_ignored_class(classes):
        if isinstance(classes, list):
            return FILTERED_OUT_CLASS in classes or CLASS_100 in classes
        return classes == FILTERED_OUT_CLASS or classes == CLASS_100

    df_filtered = df[~df['–ö–ª–∞—Å—Å'].apply(has_ignored_class)]

    found_count = 0
    detected_reviews = []

    for idx, row in tqdm(df_filtered.iterrows(), total=len(df_filtered), desc=f"üîç [{step_number}] –ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ –∏ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑ [200]"):
        review_text = str(row.get('–û—Ç–∑—ã–≤', '')).lower()

        raw_links = COMPILED_PATTERN.findall(review_text)
        raw_spam = SPAM_PATTERN.findall(review_text)

        matches_links = set(extract_matches(raw_links))
        matches_spam = set(extract_matches(raw_spam))

        all_matches = matches_links | matches_spam

        if all_matches:
            df = set_class(df, idx, 200)
            found_count += 1

            old_note = df.at[idx, '–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ'] if '–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ' in df.columns else ''
            found_str = ", ".join(sorted(all_matches))
            new_note = f"{old_note} | –Ω–∞–π–¥–µ–Ω–æ: {found_str}" if old_note else f"–Ω–∞–π–¥–µ–Ω–æ: {found_str}"
            df.at[idx, '–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ'] = new_note

            detected_reviews.append({
                '–ù–æ–º–µ—Ä —Å—Ç—Ä–æ–∫–∏': idx,
                '–û—Ç–∑—ã–≤': row.get('–û—Ç–∑—ã–≤', ''),
                '–ù–∞–π–¥–µ–Ω—ã —Å—Å—ã–ª–∫–∏/—Ñ—Ä–∞–∑—ã': found_str
            })

    logger.info(f"üìå [{step_number}] –ù–∞–π–¥–µ–Ω–æ {found_count} –æ—Ç–∑—ã–≤–æ–≤ —Å —Å—Å—ã–ª–∫–∞–º–∏ –∏–ª–∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Ñ—Ä–∞–∑–∞–º–∏. –ü—Ä–æ—Å—Ç–∞–≤–ª–µ–Ω –∫–ª–∞—Å—Å [200]")

    if SAVE_STEP_4_RESULT and found_count > 0:
        try:
            output_file = os.path.join(PROCESSED_DIR, f"step_{step_number}_classifier_200_{TIMESTAMP}.xlsx")
            df.to_excel(output_file, index=False)
            logger.info(f"üìå [{step_number}] –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {output_file}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [{step_number}] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã classifier_200: {e}")

        try:
            details_file = os.path.join(PROCESSED_DIR, f"step_4_classifier_200_details_{TIMESTAMP}.xlsx")
            pd.DataFrame(detected_reviews).to_excel(details_file, index=False)
            logger.info(f"üìÅ [{step_number}] –î–µ—Ç–∞–ª–∏ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {details_file}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [{step_number}] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–µ—Ç–∞–ª–∏ classifier_200: {e}")

        try:
            stats = class_statistics(df)
            logger.info(f"\nüìä [{step_number}] –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º –ø–æ—Å–ª–µ classifier_200:\n%s", stats.to_string(index=False))
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [{step_number}] –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–≤–µ—Å—Ç–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É classifier_200: {e}")

    return df
