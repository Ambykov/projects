# –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å cointegrated/rubert-tiny2



import os
import sys
import logging
from datetime import datetime
import random
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, roc_auc_score, confusion_matrix
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.append(PROJECT_ROOT)

# –ò–º–ø–æ—Ä—Ç—ã HuggingFace
from datasets import Dataset
from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer

# –î–ª—è –æ—Ç—á—ë—Ç–æ–≤
from docx import Document
from docx.shared import Pt, Mm
from docx.enum.text import WD_ALIGN_PARAGRAPH

# –õ–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("train_class_7_and_0")
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_handler.setFormatter(logging.Formatter("[%(asctime)s] %(levelname)s: %(message)s"))
logger.addHandler(console_handler)

# –ü—É—Ç–∏
LOGS_DIR = os.path.join(PROJECT_ROOT, "logs")
MODELS_DIR = os.path.join(PROJECT_ROOT, "models")
RESULTS_DIR = os.path.join(PROJECT_ROOT, "results")
os.makedirs(LOGS_DIR, exist_ok=True)
os.makedirs(MODELS_DIR, exist_ok=True)
os.makedirs(RESULTS_DIR, exist_ok=True)

TIMESTAMP = datetime.now().strftime("%Y%m%d_%H%M")
MODEL_SAVE_DIR = os.path.join(MODELS_DIR, f"model_class_7_and_0_{TIMESTAMP}")
RESULT_CSV = os.path.join(RESULTS_DIR, f"predictions_class_7_and_0_{TIMESTAMP}.csv")
REPORT_DOCX = os.path.join(RESULTS_DIR, f"report_class_7_and_0_{TIMESTAMP}.docx")

# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
MODEL_NAME = "cointegrated/rubert-tiny2"
MAX_LENGTH = 256
BATCH_SIZE = 16
EPOCHS = 10
LEARNING_RATE = 3e-5
SEED = 42

# –§–∏–∫—Å–∏—Ä—É–µ–º seed
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

def load_data():
    logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    DATA_PATH = os.path.join(PROJECT_ROOT, "data", "full_reviews_mixed.csv")
    try:
        df = pd.read_csv(DATA_PATH, sep=',', on_bad_lines='skip', encoding='utf-8', engine='python')
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ CSV: {e}")
        raise

    if '–ö–ª–∞—Å—Å' in df.columns and '–û—Ç–∑—ã–≤' in df.columns:
        df = df.rename(columns={"–ö–ª–∞—Å—Å": "label", "–û—Ç–∑—ã–≤": "text"})
    else:
        logger.error("‚ùå –ö–æ–ª–æ–Ω–∫–∞ '–ö–ª–∞—Å—Å' –∏–ª–∏ '–û—Ç–∑—ã–≤' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç.")
        print(df.head(3).to_string())
        raise KeyError("–§–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω –∏–ª–∏ –¥–∞–Ω–Ω—ã–µ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω—ã.")

    df['text'] = df['text'].astype(str)
    return df

def prepare_dataset(df):
    logger.info("üõ† –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")

    # –í—ã–±–∏—Ä–∞–µ–º –æ—Ç–∑—ã–≤—ã –∫–ª–∞—Å—Å–∞ 7 –∏ 0
    class_7 = df[df['label'] == 7].sample(n=min(3200, len(df[df['label'] == 7])), random_state=SEED)
    class_0 = df[df['label'] == 0].sample(n=min(3200, len(df[df['label'] == 0])), random_state=SEED)

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º
    combined = pd.concat([class_7, class_0]).sample(frac=1, random_state=SEED).reset_index(drop=True)

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–µ—Ç–∫–∏: 7 ‚Üí 1, –¥—Ä—É–≥–∏–µ ‚Üí 0
    combined['label'] = combined['label'].apply(lambda x: 1 if x == 7 else 0)

    logger.info(f"üìä –í—ã–±–æ—Ä–∫–∞: {len(combined)} –æ—Ç–∑—ã–≤–æ–≤, —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
    print(combined['label'].value_counts())

    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/test
    train_df, test_df = train_test_split(
        combined,
        test_size=0.2,
        stratify=combined['label'],
        random_state=SEED
    )

    train_dataset = Dataset.from_pandas(train_df[['text', 'label']])
    test_dataset = Dataset.from_pandas(test_df[['text', 'label']])

    return train_dataset, test_dataset

def tokenize_dataset(dataset, tokenizer):
    def tokenize_function(examples):
        texts = [str(text) for text in examples["text"]]
        return tokenizer(
            texts,
            padding="max_length",
            truncation=True,
            max_length=MAX_LENGTH,
            return_tensors="pt"
        )

    return dataset.map(tokenize_function, batched=True)

def compute_metrics(p):
    preds = p.predictions.argmax(-1)
    probs = F.softmax(torch.tensor(p.predictions), dim=-1).numpy()
    labels = p.label_ids
    return {
        "accuracy": accuracy_score(labels, preds),
        "f1": f1_score(labels, preds, average="binary"),
        "precision": precision_score(labels, preds, average="binary"),
        "recall": recall_score(labels, preds, average="binary"),
        "roc_auc": roc_auc_score(labels, probs[:, 1]),
    }

def train_model(train_dataset, eval_dataset, tokenizer):
    logger.info("üèãÔ∏è –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏...")

    model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)

    training_args = TrainingArguments(
        output_dir="./results",
        eval_strategy="epoch",
        logging_strategy="epoch",
        learning_rate=LEARNING_RATE,
        per_device_train_batch_size=BATCH_SIZE,
        num_train_epochs=EPOCHS,
        weight_decay=0.01,
        save_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="roc_auc",
        report_to="none",
        logging_dir=LOGS_DIR,
        save_total_limit=2,
        seed=SEED,
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        compute_metrics=compute_metrics
    )

    trainer.train()

    logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –≤ {MODEL_SAVE_DIR}")
    model.save_pretrained(MODEL_SAVE_DIR)
    tokenizer.save_pretrained(MODEL_SAVE_DIR)

    return trainer

def evaluate_model(trainer, test_dataset):
    logger.info("üìä –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ...")

    predictions = trainer.predict(test_dataset)
    probs = F.softmax(torch.tensor(predictions.predictions), dim=-1).numpy()
    preds = probs.argmax(-1)
    labels = predictions.label_ids

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    df_results = pd.DataFrame({
        "text": test_dataset["text"],  # –¢–µ–ø–µ—Ä—å –¥–æ—Å—Ç—É–ø–µ–Ω, —Ç–∞–∫ –∫–∞–∫ –Ω–µ —É–¥–∞–ª—ë–Ω
        "true_label": labels,
        "predicted_label": preds,
        "prob_class_7": probs[:, 1],
        "prob_class_0": probs[:, 0]
    })

    df_results.to_csv(RESULT_CSV, index=False)
    logger.info(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {RESULT_CSV}")

    # –ú–µ—Ç—Ä–∏–∫–∏
    acc = accuracy_score(labels, preds)
    f1 = f1_score(labels, preds)
    prec = precision_score(labels, preds)
    rec = recall_score(labels, preds)
    roc_auc = roc_auc_score(labels, probs[:, 1])

    print(f"üéØ –ú–µ—Ç—Ä–∏–∫–∏:")
    print(f"Accuracy: {acc:.4f}, F1: {f1:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, ROC AUC: {roc_auc:.4f}")
    print(classification_report(labels, preds, target_names=["–ö–ª–∞—Å—Å 0", "–ö–ª–∞—Å—Å 7"]))

    return df_results, probs

def plot_results(df_results, probs, save_dir, timestamp):
    logger.info("üìâ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")

    cm = confusion_matrix(df_results["true_label"], df_results["predicted_label"])
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["–ö–ª–∞—Å—Å 0", "–ö–ª–∞—Å—Å 7"],
                yticklabels=["–ö–ª–∞—Å—Å 0", "–ö–ª–∞—Å—Å 7"])
    plt.title("Confusion Matrix")
    plt.tight_layout()
    cm_path = os.path.join(save_dir, f"confusion_matrix_class_7_and_0_{timestamp}.png")
    plt.savefig(cm_path)
    plt.close()

    plt.figure(figsize=(10, 6))
    plt.hist(probs[df_results["true_label"] == 1, 1], bins=50, alpha=0.7, label="–ö–ª–∞—Å—Å 7", color="green")
    plt.hist(probs[df_results["true_label"] == 0, 1], bins=50, alpha=0.7, label="–ö–ª–∞—Å—Å 0", color="red")
    plt.title("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –¥–ª—è –∫–ª–∞—Å—Å–æ–≤")
    plt.xlabel("–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –±—ã—Ç—å –∫–ª–∞—Å—Å–æ–º 7")
    plt.ylabel("–ß–∞—Å—Ç–æ—Ç–∞")
    plt.legend()
    hist_path = os.path.join(save_dir, f"probability_histogram_class_7_and_0_{timestamp}.png")
    plt.savefig(hist_path)
    plt.close()

    fpr, tpr, _ = roc_curve(df_results["true_label"], probs[:, 1])
    roc_auc = auc(fpr, tpr)
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend(loc="lower right")
    roc_path = os.path.join(save_dir, f"roc_curve_class_7_and_0_{timestamp}.png")
    plt.savefig(roc_path)
    plt.close()

    return cm_path, hist_path, roc_path

def save_report_to_docx(df_results, metrics, cm_path, hist_path, roc_path, save_dir, timestamp):
    logger.info("üìÑ –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á—ë—Ç–∞ –≤ Word (.docx)...")

    doc = Document()
    doc.add_heading("–û—Ç—á—ë—Ç –ø–æ –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–∏: –ö–ª–∞—Å—Å 7 vs –ö–ª–∞—Å—Å 0", level=1)
    doc.add_paragraph(f"–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è: {timestamp}")

    doc.add_heading("–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞", level=2)
    table = doc.add_table(rows=1, cols=2)
    table.style = "Table Grid"
    hdr_cells = table.rows[0].cells
    hdr_cells[0].text = "–ú–µ—Ç—Ä–∏–∫–∞"
    hdr_cells[1].text = "–ó–Ω–∞—á–µ–Ω–∏–µ"

    for key, value in metrics.items():
        row_cells = table.add_row().cells
        row_cells[0].text = key
        row_cells[1].text = f"{value:.4f}"

    doc.add_heading("–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç—á—ë—Ç", level=2)
    report = classification_report(df_results['true_label'], df_results['predicted_label'],
                                  target_names=["–ö–ª–∞—Å—Å 0", "–ö–ª–∞—Å—Å 7"], digits=4)
    doc.add_paragraph(report)

    doc.add_heading("–ü—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π", level=2)
    sample = df_results.head(10)
    table = doc.add_table(rows=1, cols=5)
    hdr_cells = table.rows[0].cells
    hdr_cells[0].text = "–¢–µ–∫—Å—Ç"
    hdr_cells[1].text = "–ú–µ—Ç–∫–∞"
    hdr_cells[2].text = "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"
    hdr_cells[3].text = "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å (–ö–ª–∞—Å—Å 7)"
    hdr_cells[4].text = "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å (–ö–ª–∞—Å—Å 0)"

    for _, row in sample.iterrows():
        row_cells = table.add_row().cells
        row_cells[0].text = row['text'][:100] + "..." if len(row['text']) > 100 else row['text']
        row_cells[1].text = str(row['true_label'])
        row_cells[2].text = str(row['predicted_label'])
        row_cells[3].text = f"{row['prob_class_7']:.4f}"
        row_cells[4].text = f"{row['prob_class_0']:.4f}"

    doc.add_heading("–ì—Ä–∞—Ñ–∏–∫–∏", level=2)
    doc.add_picture(cm_path, width=Mm(150))
    doc.add_paragraph("–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫", style='Caption')
    doc.add_picture(hist_path, width=Mm(150))
    doc.add_paragraph("–ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π", style='Caption')
    doc.add_picture(roc_path, width=Mm(150))
    doc.add_paragraph("ROC-–∫—Ä–∏–≤–∞—è", style='Caption')

    report_path = os.path.join(save_dir, f"report_class_7_and_0_{timestamp}.docx")
    doc.save(report_path)
    logger.info(f"‚úÖ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {report_path}")
    return report_path

if __name__ == "__main__":
    logger.info("üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–ª–∞—Å—Å–æ–≤ 7 –∏ 0")

    df = load_data()
    train_dataset, test_dataset = prepare_dataset(df)

    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)
    tokenized_train = tokenize_dataset(train_dataset, tokenizer)
    tokenized_test = tokenize_dataset(test_dataset, tokenizer).with_format("torch")

    trainer = train_model(tokenized_train, tokenized_test, tokenizer)
    df_results, probs = evaluate_model(trainer, tokenized_test)

    cm_path, hist_path, roc_path = plot_results(df_results, probs, RESULTS_DIR, TIMESTAMP)

    metrics_dict = {
        "accuracy": accuracy_score(df_results['true_label'], df_results['predicted_label']),
        "f1": f1_score(df_results['true_label'], df_results['predicted_label']),
        "precision": precision_score(df_results['true_label'], df_results['predicted_label']),
        "recall": recall_score(df_results['true_label'], df_results['predicted_label']),
        "roc_auc": roc_auc_score(df_results['true_label'], df_results['prob_class_7']),
        "loss": trainer.evaluate().get("eval_loss", 0)
    }

    save_report_to_docx(df_results, metrics_dict, cm_path, hist_path, roc_path, RESULTS_DIR, TIMESTAMP)

    logger.info("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∏ –æ—Ç—á—ë—Ç –∑–∞–≤–µ—Ä—à–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")