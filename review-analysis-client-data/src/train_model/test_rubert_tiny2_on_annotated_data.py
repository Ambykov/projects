import os
import sys
import logging
from datetime import datetime
import random
import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.append(PROJECT_ROOT)

# –ò–º–ø–æ—Ä—Ç—ã HuggingFace
from datasets import Dataset
from transformers import BertTokenizer, BertForSequenceClassification

# –î–ª—è –æ—Ç—á—ë—Ç–æ–≤
from docx import Document
from docx.shared import Pt, Mm
from docx.enum.text import WD_ALIGN_PARAGRAPH

# –ú–µ—Ç—Ä–∏–∫–∏
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, roc_auc_score
from sklearn.metrics import confusion_matrix, roc_curve, auc

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
import matplotlib.pyplot as plt
import seaborn as sns

# –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
from pymorphy3 import MorphAnalyzer
import re

# –õ–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("test_model")
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_handler.setFormatter(logging.Formatter("[%(asctime)s] %(levelname)s: %(message)s"))
logger.addHandler(console_handler)

# –ü—É—Ç–∏
LOGS_DIR = os.path.join(PROJECT_ROOT, "logs")
MODELS_DIR = os.path.join(PROJECT_ROOT, "models")
RESULTS_DIR = os.path.join(PROJECT_ROOT, "results")
os.makedirs(LOGS_DIR, exist_ok=True)
os.makedirs(MODELS_DIR, exist_ok=True)
os.makedirs(RESULTS_DIR, exist_ok=True)

TIMESTAMP = datetime.now().strftime("%Y%m%d_%H%M")
RESULT_CSV = os.path.join(RESULTS_DIR, f"predictions_test_class_7_and_other_{TIMESTAMP}.csv")
REPORT_DOCX = os.path.join(RESULTS_DIR, f"report_test_class_7_and_other_{TIMESTAMP}.docx")

# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
MODEL_NAME = "cointegrated/rubert-tiny2"
MAX_LENGTH = 256
BATCH_SIZE = 16
SEED = 42

# –§–∏–∫—Å–∏—Ä—É–µ–º seed
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ª–µ–º–º–∞—Ç–∏–∑–∞—Ç–æ—Ä
morph = MorphAnalyzer()

def preprocess_text(text):
    """–ü—Ä–∏–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É, —É–¥–∞–ª—è–µ—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é, –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç"""
    text = str(text).lower()
    text = re.sub(r'[^\w\s]', '', text)  # –£–¥–∞–ª—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
    text = re.sub(r'\s+', ' ', text).strip()  # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    tokens = text.split()
    lemmatized = [morph.parse(token)[0].normal_form for token in tokens if token.isalpha()]
    return ' '.join(lemmatized)

def load_data():
    logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    DATA_PATH = os.path.join(PROJECT_ROOT, "data", "ALL_Reviews_P23.csv")
    df = pd.read_csv(DATA_PATH, sep=',', on_bad_lines='skip', encoding='utf-8', engine='python')

    if '–ö–ª–∞—Å—Å' in df.columns and '–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞' in df.columns:
        df = df.rename(columns={"–ö–ª–∞—Å—Å": "label", "–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞": "text"})
    else:
        logger.error("‚ùå –ö–æ–ª–æ–Ω–∫–∞ '–ö–ª–∞—Å—Å' –∏–ª–∏ '–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç.")
        print(df.head(3).to_string())
        raise KeyError("–§–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω –∏–ª–∏ –¥–∞–Ω–Ω—ã–µ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω—ã.")

    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É –∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é
    df['text'] = df['text'].astype(str).apply(preprocess_text)
    df['label'] = df['label'].apply(lambda x: 1 if x == 7 else 0)

    return df

def prepare_dataset(df):
    logger.info("üõ† –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")

    # –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: –ø–æ 2300 –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
    class_7 = df[df['label'] == 1].sample(n=2300, random_state=SEED)
    class_0 = df[df['label'] == 0].sample(n=2300, random_state=SEED)

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º
    combined = pd.concat([class_7, class_0]).sample(frac=1, random_state=SEED).reset_index(drop=True)

    logger.info(f"üìä –í—ã–±–æ—Ä–∫–∞: {len(combined)} –æ—Ç–∑—ã–≤–æ–≤, —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
    print(combined['label'].value_counts())

    test_dataset = Dataset.from_pandas(combined[['text', 'label']])
    return test_dataset

def tokenize_dataset(dataset, tokenizer):
    def tokenize_function(examples):
        texts = [str(text) for text in examples["text"]]
        return tokenizer(
            texts,
            padding="max_length",
            truncation=True,
            max_length=MAX_LENGTH,
            return_tensors="pt"
        )

    return dataset.map(tokenize_function, batched=True, remove_columns=["text"])

def evaluate_model(model, tokenizer, test_dataset):
    logger.info("üìä –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ...")

    tokenized_test = tokenize_dataset(test_dataset, tokenizer).with_format("torch")
    test_loader = DataLoader(tokenized_test, batch_size=BATCH_SIZE)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()

    true_labels = []
    predictions = []
    probs_list = []

    with torch.no_grad():
        for batch in test_loader:
            inputs = {k: v.to(device) for k, v in batch.items() if k in ["input_ids", "attention_mask"]}
            labels = batch["label"].to(device)

            outputs = model(**inputs)
            logits = outputs.logits
            probs = F.softmax(logits, dim=-1)
            preds = torch.argmax(logits, dim=-1)

            true_labels.extend(labels.cpu().numpy())
            predictions.extend(preds.cpu().numpy())
            probs_list.extend(probs.cpu().numpy())

    probs_array = np.array(probs_list)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    df_results = pd.DataFrame({
        "text": test_dataset["text"],
        "true_label": true_labels,
        "predicted_label": predictions,
        "prob_class_7": probs_array[:, 1],
        "prob_class_other": probs_array[:, 0]
    })

    df_results.to_csv(RESULT_CSV, index=False)
    logger.info(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {RESULT_CSV}")

    # –ú–µ—Ç—Ä–∏–∫–∏
    acc = accuracy_score(true_labels, predictions)
    f1 = f1_score(true_labels, predictions)
    prec = precision_score(true_labels, predictions)
    rec = recall_score(true_labels, predictions)
    roc_auc = roc_auc_score(true_labels, probs_array[:, 1])

    print(f"üéØ –ú–µ—Ç—Ä–∏–∫–∏:")
    print(f"Accuracy: {acc:.4f}, F1: {f1:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, ROC AUC: {roc_auc:.4f}")
    print(classification_report(true_labels, predictions, target_names=["–ö–ª–∞—Å—Å 0", "–ö–ª–∞—Å—Å 7"]))

    return df_results, true_labels, predictions, probs_array

def plot_results(true_labels, probs, save_dir, timestamp):
    logger.info("üìâ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤...")

    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
    cm = confusion_matrix(true_labels, np.argmax(probs, axis=1))
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["–ö–ª–∞—Å—Å 0", "–ö–ª–∞—Å—Å 7"],
                yticklabels=["–ö–ª–∞—Å—Å 0", "–ö–ª–∞—Å—Å 7"])
    plt.title("Confusion Matrix")
    plt.tight_layout()
    cm_path = os.path.join(save_dir, f"confusion_matrix_class_7_and_other_{timestamp}.png")
    plt.savefig(cm_path)
    plt.close()

    # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
    plt.figure(figsize=(10, 6))
    plt.hist(probs[np.array(true_labels) == 1, 1], bins=50, alpha=0.7, label="–ö–ª–∞—Å—Å 7", color="green")
    plt.hist(probs[np.array(true_labels) == 0, 1], bins=50, alpha=0.7, label="–ö–ª–∞—Å—Å 0", color="red")
    plt.title("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –¥–ª—è –∫–ª–∞—Å—Å–æ–≤")
    plt.xlabel("–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –±—ã—Ç—å –∫–ª–∞—Å—Å–æ–º 7")
    plt.ylabel("–ß–∞—Å—Ç–æ—Ç–∞")
    plt.legend()
    hist_path = os.path.join(save_dir, f"probability_histogram_class_7_and_other_{timestamp}.png")
    plt.savefig(hist_path)
    plt.close()

    # ROC-–∫—Ä–∏–≤–∞—è
    fpr, tpr, _ = roc_curve(true_labels, probs[:, 1])
    roc_auc = auc(fpr, tpr)
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend(loc="lower right")
    roc_path = os.path.join(save_dir, f"roc_curve_class_7_and_other_{timestamp}.png")
    plt.savefig(roc_path)
    plt.close()

    return cm_path, hist_path, roc_path

def save_report_to_docx(df_results, true_labels, predictions, probs, cm_path, hist_path, roc_path, save_dir, timestamp):
    logger.info("üìÑ –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á—ë—Ç–∞ –≤ Word (.docx)...")

    doc = Document()
    doc.add_heading("–û—Ç—á—ë—Ç –ø–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –º–æ–¥–µ–ª–∏: –ö–ª–∞—Å—Å 7 vs –ö–ª–∞—Å—Å 0", level=1)
    doc.add_paragraph(f"–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è: {timestamp}")

    # –ú–µ—Ç—Ä–∏–∫–∏
    table = doc.add_table(rows=1, cols=2)
    table.style = "Table Grid"
    hdr_cells = table.rows[0].cells
    hdr_cells[0].text = "–ú–µ—Ç—Ä–∏–∫–∞"
    hdr_cells[1].text = "–ó–Ω–∞—á–µ–Ω–∏–µ"

    metrics = {
        "accuracy": accuracy_score(true_labels, predictions),
        "f1": f1_score(true_labels, predictions),
        "precision": precision_score(true_labels, predictions),
        "recall": recall_score(true_labels, predictions),
        "roc_auc": roc_auc_score(true_labels, probs[:, 1]),
    }

    for key, value in metrics.items():
        row_cells = table.add_row().cells
        row_cells[0].text = key
        row_cells[1].text = f"{value:.4f}"

    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç—á—ë—Ç
    doc.add_heading("–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç—á—ë—Ç", level=2)
    report = classification_report(true_labels, predictions, target_names=["–ö–ª–∞—Å—Å 0", "–ö–ª–∞—Å—Å 7"], digits=4)
    doc.add_paragraph(report)

    # –ü—Ä–∏–º–µ—Ä—ã
    doc.add_heading("–ü—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π", level=2)
    sample = df_results.head(10)
    table = doc.add_table(rows=1, cols=5)
    hdr_cells = table.rows[0].cells
    hdr_cells[0].text = "–¢–µ–∫—Å—Ç"
    hdr_cells[1].text = "–ú–µ—Ç–∫–∞"
    hdr_cells[2].text = "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"
    hdr_cells[3].text = "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å (–ö–ª–∞—Å—Å 7)"
    hdr_cells[4].text = "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å (–ö–ª–∞—Å—Å 0)"
    for _, row in sample.iterrows():
        row_cells = table.add_row().cells
        row_cells[0].text = row['text'][:100] + "..." if len(row['text']) > 100 else row['text']
        row_cells[1].text = str(row['true_label'])
        row_cells[2].text = str(row['predicted_label'])
        row_cells[3].text = f"{row['prob_class_7']:.4f}"
        row_cells[4].text = f"{row['prob_class_other']:.4f}"

    # –ì—Ä–∞—Ñ–∏–∫–∏
    doc.add_heading("–ì—Ä–∞—Ñ–∏–∫–∏", level=2)
    doc.add_picture(cm_path, width=Mm(150))
    doc.add_paragraph("–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫", style='Caption')
    doc.add_picture(hist_path, width=Mm(150))
    doc.add_paragraph("–ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π", style='Caption')
    doc.add_picture(roc_path, width=Mm(150))
    doc.add_paragraph("ROC-–∫—Ä–∏–≤–∞—è", style='Caption')

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á—ë—Ç
    report_path = os.path.join(save_dir, f"report_test_class_7_and_other_{timestamp}.docx")
    doc.save(report_path)
    logger.info(f"‚úÖ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {report_path}")
    return report_path

if __name__ == "__main__":
    logger.info("üöÄ –ù–∞—á–∞–ª–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏ –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º
    df = load_data()

    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    test_dataset = prepare_dataset(df)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    #MODEL_SAVE_DIR = r"./models/model_class_7_and_0_20250721_1127"
    MODEL_SAVE_DIR = r"./models/model_class_7_and_0_20250728_0702"
    logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ {MODEL_SAVE_DIR}")

    try:
        tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)
        model = BertForSequenceClassification.from_pretrained(MODEL_SAVE_DIR, num_labels=2)
    except Exception as e:
        logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å: {e}")
        raise

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
    df_results, true_labels, predictions, probs = evaluate_model(model, tokenizer, test_dataset)

    # –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫–∏
    cm_path, hist_path, roc_path = plot_results(true_labels, probs, RESULTS_DIR, TIMESTAMP)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á—ë—Ç –≤ Word
    save_report_to_docx(df_results, true_labels, predictions, probs, cm_path, hist_path, roc_path, RESULTS_DIR, TIMESTAMP)

    logger.info("‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")