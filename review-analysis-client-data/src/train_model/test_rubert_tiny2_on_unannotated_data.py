import os
import sys
import logging
from datetime import datetime
import random
import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.append(PROJECT_ROOT)

# –ò–º–ø–æ—Ä—Ç—ã HuggingFace
from datasets import Dataset
from transformers import BertTokenizer, BertForSequenceClassification

# –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
from pymorphy3 import MorphAnalyzer
import re

# –î–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
import matplotlib.pyplot as plt
import seaborn as sns

# –õ–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("classify_unlabeled")
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_handler.setFormatter(logging.Formatter("[%(asctime)s] %(levelname)s: %(message)s"))
logger.addHandler(console_handler)

# –ü—É—Ç–∏
LOGS_DIR = os.path.join(PROJECT_ROOT, "logs")
RESULTS_DIR = os.path.join(PROJECT_ROOT, "results")
os.makedirs(LOGS_DIR, exist_ok=True)
os.makedirs(RESULTS_DIR, exist_ok=True)

TIMESTAMP = datetime.now().strftime("%Y%m%d_%H%M")
RESULT_CSV = os.path.join(RESULTS_DIR, f"unlabeled_classification_results_{TIMESTAMP}.csv")
RESULT_STATISTICS = os.path.join(RESULTS_DIR, f"statistics_{TIMESTAMP}.txt")
RESULT_PLOT = os.path.join(RESULTS_DIR, f"class_distribution_{TIMESTAMP}.png")

# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
MODEL_NAME = "cointegrated/rubert-tiny2"
MAX_LENGTH = 256
BATCH_SIZE = 16
SEED = 42

# –§–∏–∫—Å–∏—Ä—É–µ–º seed
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ª–µ–º–º–∞—Ç–∏–∑–∞—Ç–æ—Ä
morph = MorphAnalyzer()

def preprocess_text(text):
    """–ü—Ä–∏–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É, —É–¥–∞–ª—è–µ—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é, –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç"""
    text = str(text).lower()
    text = re.sub(r'[^\w\s]', '', text)  # —É–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
    text = re.sub(r'\s+', ' ', text).strip()
    tokens = text.split()
    lemmatized_tokens = [morph.parse(token)[0].normal_form for token in tokens if token.isalpha()]
    return ' '.join(lemmatized_tokens)

def load_unlabeled_data():
    logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–µ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ Excel...")
    DATA_PATH = os.path.join(PROJECT_ROOT, "data", "format2_reviews.xlsx")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    df = pd.read_excel(DATA_PATH)

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–æ–ª—è
    df["–û—Ç–∑—ã–≤"] = df["–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞"].fillna("") + " " + df["–î–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤–∞"].fillna("") + " " + df["–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏"].fillna("")
    df["–û—Ç–∑—ã–≤"] = df["–û—Ç–∑—ã–≤"].apply(preprocess_text)

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ ID –∏ —Ç–µ–∫—Å—Ç
    return df[["ID –æ—Ç–∑—ã–≤–∞", "–û—Ç–∑—ã–≤"]]

def tokenize_dataset(dataset, tokenizer):
    def tokenize_function(examples):
        texts = [str(text) for text in examples["–û—Ç–∑—ã–≤"]]
        return tokenizer(
            texts,
            padding="max_length",
            truncation=True,
            max_length=MAX_LENGTH,
            return_tensors="pt"
        )
    return dataset.map(tokenize_function, batched=True, remove_columns=["–û—Ç–∑—ã–≤"])

def classify_reviews(model, tokenizer, reviews_df):
    logger.info("üìä –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–µ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤...")

    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    tokenized_reviews = Dataset.from_pandas(reviews_df[['–û—Ç–∑—ã–≤']])
    tokenized_reviews = tokenized_reviews.map(
        lambda x: tokenizer(x['–û—Ç–∑—ã–≤'], padding="max_length", truncation=True, max_length=MAX_LENGTH),
        batched=True, remove_columns=["–û—Ç–∑—ã–≤"]
    ).with_format("torch")

    test_loader = DataLoader(tokenized_reviews, batch_size=BATCH_SIZE)

    # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()

    predictions = []
    probabilities = []

    with torch.no_grad():
        for batch in test_loader:
            inputs = {k: v.to(device) for k, v in batch.items() if k in ["input_ids", "attention_mask"]}
            outputs = model(**inputs)
            logits = outputs.logits
            probs = F.softmax(logits, dim=-1)
            preds = torch.argmax(logits, dim=-1)

            predictions.extend(preds.cpu().numpy())
            probabilities.extend(probs.cpu().numpy())

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ DataFrame
    reviews_df["predicted_label"] = predictions
    reviews_df["prob_class_7"] = [p[1].item() for p in probabilities]
    reviews_df["prob_class_0"] = [p[0].item() for p in probabilities]

    return reviews_df

def save_results(df_results, result_csv):
    logger.info(f"üìÅ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ {result_csv}...")
    df_results.to_csv(result_csv, index=False)
    logger.info(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {result_csv}")

def print_statistics(df_results):
    total_reviews = len(df_results)
    class_7_count = (df_results["predicted_label"] == 1).sum()
    class_0_count = (df_results["predicted_label"] == 0).sum()

    class_7_percentage = (class_7_count / total_reviews) * 100
    class_0_percentage = (class_0_count / total_reviews) * 100

    logger.info("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:")
    logger.info(f"–í—Å–µ–≥–æ –æ—Ç–∑—ã–≤–æ–≤: {total_reviews}")
    logger.info(f"–ö–ª–∞—Å—Å 7: {class_7_count} ({class_7_percentage:.2f}%)")
    logger.info(f"–ö–ª–∞—Å—Å 0: {class_0_count} ({class_0_percentage:.2f}%)")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ —Ñ–∞–π–ª
    with open(RESULT_STATISTICS, "w", encoding="utf-8") as f:
        f.write(f"–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è: {TIMESTAMP}\n")
        f.write(f"–í—Å–µ–≥–æ –æ—Ç–∑—ã–≤–æ–≤: {total_reviews}\n")
        f.write(f"–ö–ª–∞—Å—Å 7: {class_7_count} ({class_7_percentage:.2f}%)\n")
        f.write(f"–ö–ª–∞—Å—Å 0: {class_0_count} ({class_0_percentage:.2f}%)\n")

    return {
        "total_reviews": total_reviews,
        "class_7_count": class_7_count,
        "class_0_count": class_0_count,
        "class_7_percentage": class_7_percentage,
        "class_0_percentage": class_0_percentage
    }

def plot_distribution(df_results):
    logger.info("üìâ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤...")

    plt.figure(figsize=(8, 5))
    df_results["predicted_label"].value_counts().plot(kind='bar', color=['#ff7f0e', '#1f77b4'])
    plt.title("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ —Å—Ä–µ–¥–∏ –Ω–µ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤")
    plt.xlabel("–ö–ª–∞—Å—Å")
    plt.ylabel("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ")
    plt.xticks([0, 1], ["–ö–ª–∞—Å—Å 0", "–ö–ª–∞—Å—Å 7"], rotation=0)
    plt.tight_layout()
    plt.savefig(RESULT_PLOT)
    plt.close()
    logger.info(f"‚úÖ –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {RESULT_PLOT}")
    return RESULT_PLOT

if __name__ == "__main__":
    logger.info("üöÄ –ù–∞—á–∞–ª–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–µ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–µ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    df = load_unlabeled_data()

    # –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
    #ODEL_SAVE_DIR = r"./models/model_class_7_and_0_20250721_1127"
    MODEL_SAVE_DIR = r"./models/model_class_7_and_0_20250728_0702"
    logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ {MODEL_SAVE_DIR}")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    try:
        tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)
        model = BertForSequenceClassification.from_pretrained(MODEL_SAVE_DIR, num_labels=2)
    except Exception as e:
        logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å: {e}")
        raise

    # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –æ—Ç–∑—ã–≤—ã
    classified_df = classify_reviews(model, tokenizer, df)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    save_results(classified_df, RESULT_CSV)

    # –í—ã–≤–æ–¥–∏–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    stats = print_statistics(classified_df)

    # –°—Ç—Ä–æ–∏–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫
    plot_distribution(classified_df)

    logger.info("‚úÖ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–µ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")